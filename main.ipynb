{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad67453",
   "metadata": {},
   "source": [
    "\n",
    "# DataLog - Solução de Previsão de Demanda com Machine Learning\n",
    "\n",
    "- Este código implementa uma solução completa para previsão de demanda logística utilizando técnicas de aprendizado de máquina, conforme solicitado na Avaliação A3.\n",
    "\n",
    "## Autores: Equipe DataLog Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99a17539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Importando bibliotecas necessárias \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBRegressor\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import warnings\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8e877c",
   "metadata": {},
   "source": [
    "# Principais bibliotecas utilizadas\n",
    "- pandas: manipulação e análise de dados em tabelas.\n",
    "- numpy: operações matemáticas e manipulação eficiente de arrays.\n",
    "- matplotlib.pyplot e seaborn: criação de gráficos e visualizações de dados.\n",
    "- datetime e timedelta: manipulação de datas e tempos.\n",
    "- random: geração de números aleatórios.\n",
    "- scikit-learn: ferramentas para machine learning.\n",
    "- warnings: controle de avisos do Python.\n",
    "- os: manipulação de diretórios e arquivos do sistema operacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8ec8f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cofigurações Iniciais\"\"\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "\"\"\" Criação de diretório de trabalho \"\"\"\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d32326",
   "metadata": {},
   "source": [
    "# Início da Etapa 1 - Coleta e preparação de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a308eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Etapa 1 - Coleta e preparação de dados \n",
    "    - Gerando dados sintéticos para simular a demanda de produtos em diferentes centros de distribuição (CDs)\n",
    "    - Definindo características dos CDs, eventos especiais e categorias de produtos\n",
    "    - Definindo processos logísticos por CD\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "CDS = {\n",
    "    \"Belem (PA)\": {\"lat\": -1.4558, \"lon\": -48.5044, \"capacidade_diaria\": 1200},\n",
    "    \"Recife (PE)\": {\"lat\": -8.0476, \"lon\": -34.8770, \"capacidade_diaria\": 1800},\n",
    "    \"Brasilia (DF)\": {\"lat\": -15.7942, \"lon\": -47.8822, \"capacidade_diaria\": 2000},\n",
    "    \"Sao Paulo (SP)\": {\"lat\": -23.5505, \"lon\": -46.6333, \"capacidade_diaria\": 5000},\n",
    "    \"Florianopolis (SC)\": {\"lat\": -27.5954, \"lon\": -48.5480, \"capacidade_diaria\": 1500}\n",
    "}\n",
    "\n",
    "# Definição de eventos especiais que afetam a demanda\n",
    "EVENTOS_ESPECIAIS = {\n",
    "    \"Black Friday\": {\"data_inicio\": \"2024-11-25\", \"data_fim\": \"2024-11-30\", \"impacto\": 2.5},\n",
    "    \"Natal\": {\"data_inicio\": \"2024-12-15\", \"data_fim\": \"2024-12-24\", \"impacto\": 2.0},\n",
    "    \"Dia das Mães\": {\"data_inicio\": \"2024-05-05\", \"data_fim\": \"2024-05-12\", \"impacto\": 1.8},\n",
    "    \"Dia dos Pais\": {\"data_inicio\": \"2024-08-04\", \"data_fim\": \"2024-08-11\", \"impacto\": 1.5},\n",
    "    \"Dia das Crianças\": {\"data_inicio\": \"2024-10-05\", \"data_fim\": \"2024-10-12\", \"impacto\": 1.7},\n",
    "    \"Carnaval\": {\"data_inicio\": \"2024-02-10\", \"data_fim\": \"2024-02-14\", \"impacto\": 0.7},  # Redução na demanda\n",
    "    \"Cyber Monday\": {\"data_inicio\": \"2024-12-02\", \"data_fim\": \"2024-12-02\", \"impacto\": 2.0},\n",
    "    \"Volta às Aulas\": {\"data_inicio\": \"2024-01-15\", \"data_fim\": \"2024-02-05\", \"impacto\": 1.6}\n",
    "}\n",
    "\n",
    "# Definição de categorias de produtos\n",
    "CATEGORIAS_PRODUTOS = {\n",
    "    \"Eletrônicos\": {\"peso_medio\": 3.5, \"volume_medio\": 0.015, \"valor_medio\": 1200, \"sazonalidade\": \"alta\",\n",
    "                    \"complexidade_picking\": \"alta\", \"fragilidade\": \"alta\", \"tempo_base_picking\": 4.5,\n",
    "                    \"tempo_base_embalagem\": 3.5, \"prioridade_expedicao\": \"media\"},\n",
    "    \"Vestuário\": {\"peso_medio\": 0.8, \"volume_medio\": 0.005, \"valor_medio\": 150, \"sazonalidade\": \"media\",\n",
    "                 \"complexidade_picking\": \"media\", \"fragilidade\": \"baixa\", \"tempo_base_picking\": 2.5,\n",
    "                 \"tempo_base_embalagem\": 1.5, \"prioridade_expedicao\": \"baixa\"},\n",
    "    \"Alimentos\": {\"peso_medio\": 5.0, \"volume_medio\": 0.02, \"valor_medio\": 80, \"sazonalidade\": \"baixa\",\n",
    "                 \"complexidade_picking\": \"baixa\", \"fragilidade\": \"media\", \"tempo_base_picking\": 2.0,\n",
    "                 \"tempo_base_embalagem\": 1.8, \"prioridade_expedicao\": \"alta\"},\n",
    "    \"Casa e Decoração\": {\"peso_medio\": 8.0, \"volume_medio\": 0.05, \"valor_medio\": 300, \"sazonalidade\": \"media\",\n",
    "                        \"complexidade_picking\": \"alta\", \"fragilidade\": \"alta\", \"tempo_base_picking\": 5.0,\n",
    "                        \"tempo_base_embalagem\": 4.0, \"prioridade_expedicao\": \"baixa\"},\n",
    "    \"Beleza e Saúde\": {\"peso_medio\": 1.2, \"volume_medio\": 0.008, \"valor_medio\": 120, \"sazonalidade\": \"baixa\",\n",
    "                      \"complexidade_picking\": \"media\", \"fragilidade\": \"media\", \"tempo_base_picking\": 3.0,\n",
    "                      \"tempo_base_embalagem\": 2.0, \"prioridade_expedicao\": \"media\"},\n",
    "    \"Esportes\": {\"peso_medio\": 4.0, \"volume_medio\": 0.025, \"valor_medio\": 250, \"sazonalidade\": \"alta\",\n",
    "                \"complexidade_picking\": \"alta\", \"fragilidade\": \"media\", \"tempo_base_picking\": 3.5,\n",
    "                \"tempo_base_embalagem\": 2.5, \"prioridade_expedicao\": \"baixa\"},\n",
    "    \"Livros e Mídia\": {\"peso_medio\": 1.0, \"volume_medio\": 0.004, \"valor_medio\": 70, \"sazonalidade\": \"media\",\n",
    "                      \"complexidade_picking\": \"baixa\", \"fragilidade\": \"baixa\", \"tempo_base_picking\": 1.5,\n",
    "                      \"tempo_base_embalagem\": 1.0, \"prioridade_expedicao\": \"baixa\"},\n",
    "    \"Brinquedos\": {\"peso_medio\": 2.0, \"volume_medio\": 0.018, \"valor_medio\": 100, \"sazonalidade\": \"alta\",\n",
    "                  \"complexidade_picking\": \"media\", \"fragilidade\": \"media\", \"tempo_base_picking\": 3.0,\n",
    "                  \"tempo_base_embalagem\": 2.5, \"prioridade_expedicao\": \"media\"}\n",
    "}\n",
    "\n",
    "# Definição de características dos processos logísticos por CD\n",
    "PROCESSOS_LOGISTICOS_CD = {\n",
    "    \"Belem (PA)\": {\n",
    "        \"eficiencia_picking\": 0.85,  # Eficiência relativa do processo de picking (1.0 = 100%)\n",
    "        \"eficiencia_embalagem\": 0.80,  # Eficiência relativa do processo de embalagem\n",
    "        \"eficiencia_expedicao\": 0.75,  # Eficiência relativa do processo de expedição\n",
    "        \"taxa_erro_picking_base\": 0.05,  # Taxa base de erro no picking\n",
    "        \"taxa_erro_embalagem_base\": 0.03,  # Taxa base de erro na embalagem\n",
    "        \"tempo_medio_expedicao\": 120,  # Tempo médio de expedição em minutos\n",
    "        \"nivel_automacao\": \"baixo\",  # Nível de automação do CD\n",
    "        \"capacidade_picking_hora\": 80,  # Itens que podem ser separados por hora\n",
    "        \"capacidade_embalagem_hora\": 60,  # Itens que podem ser embalados por hora\n",
    "        \"capacidade_expedicao_hora\": 40  # Pedidos que podem ser expedidos por hora\n",
    "    },\n",
    "    \"Recife (PE)\": {\n",
    "        \"eficiencia_picking\": 0.90,\n",
    "        \"eficiencia_embalagem\": 0.85,\n",
    "        \"eficiencia_expedicao\": 0.80,\n",
    "        \"taxa_erro_picking_base\": 0.04,\n",
    "        \"taxa_erro_embalagem_base\": 0.025,\n",
    "        \"tempo_medio_expedicao\": 100,\n",
    "        \"nivel_automacao\": \"medio\",\n",
    "        \"capacidade_picking_hora\": 100,\n",
    "        \"capacidade_embalagem_hora\": 80,\n",
    "        \"capacidade_expedicao_hora\": 50\n",
    "    },\n",
    "    \"Brasilia (DF)\": {\n",
    "        \"eficiencia_picking\": 0.92,\n",
    "        \"eficiencia_embalagem\": 0.88,\n",
    "        \"eficiencia_expedicao\": 0.85,\n",
    "        \"taxa_erro_picking_base\": 0.035,\n",
    "        \"taxa_erro_embalagem_base\": 0.02,\n",
    "        \"tempo_medio_expedicao\": 90,\n",
    "        \"nivel_automacao\": \"medio\",\n",
    "        \"capacidade_picking_hora\": 120,\n",
    "        \"capacidade_embalagem_hora\": 90,\n",
    "        \"capacidade_expedicao_hora\": 60\n",
    "    },\n",
    "    \"Sao Paulo (SP)\": {\n",
    "        \"eficiencia_picking\": 0.95,\n",
    "        \"eficiencia_embalagem\": 0.93,\n",
    "        \"eficiencia_expedicao\": 0.90,\n",
    "        \"taxa_erro_picking_base\": 0.025,\n",
    "        \"taxa_erro_embalagem_base\": 0.015,\n",
    "        \"tempo_medio_expedicao\": 60,\n",
    "        \"nivel_automacao\": \"alto\",\n",
    "        \"capacidade_picking_hora\": 200,\n",
    "        \"capacidade_embalagem_hora\": 150,\n",
    "        \"capacidade_expedicao_hora\": 100\n",
    "    },\n",
    "    \"Florianopolis (SC)\": {\n",
    "        \"eficiencia_picking\": 0.88,\n",
    "        \"eficiencia_embalagem\": 0.86,\n",
    "        \"eficiencia_expedicao\": 0.82,\n",
    "        \"taxa_erro_picking_base\": 0.03,\n",
    "        \"taxa_erro_embalagem_base\": 0.02,\n",
    "        \"tempo_medio_expedicao\": 80,\n",
    "        \"nivel_automacao\": \"medio\",\n",
    "        \"capacidade_picking_hora\": 110,\n",
    "        \"capacidade_embalagem_hora\": 85,\n",
    "        \"capacidade_expedicao_hora\": 55\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb3deaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Etapa 1 - Coleta e preparação de dados\n",
    "- Validação da estrutura de dados\n",
    "- geração de dados sintéticos\n",
    "\"\"\"\n",
    "\n",
    "# Função para verificar se uma data está dentro de um evento especial\n",
    "def verificar_evento_especial(data):\n",
    "    \"\"\"\n",
    "    Verifica se uma data específica está dentro de algum evento especial.\n",
    "\n",
    "    Args:\n",
    "        data: Data a ser verificada\n",
    "\n",
    "    Returns:\n",
    "        Tupla com nome do evento e fator de impacto, ou (None, 1.0) se não houver evento\n",
    "    \"\"\"\n",
    "    for evento, info in EVENTOS_ESPECIAIS.items():\n",
    "        data_inicio = pd.to_datetime(info[\"data_inicio\"])\n",
    "        data_fim = pd.to_datetime(info[\"data_fim\"])\n",
    "\n",
    "        if data_inicio <= data <= data_fim:\n",
    "            return evento, info[\"impacto\"]\n",
    "\n",
    "    return None, 1.0\n",
    "\n",
    "\n",
    "# Função para gerar dados sintéticos de demanda logística\n",
    "def gerar_dados_demanda(dias=365, pedidos_base=50000):\n",
    "    \"\"\"\n",
    "    Gera dados sintéticos de demanda logística para análise e modelagem.\n",
    "\n",
    "    Args:\n",
    "        dias: Número de dias para gerar dados\n",
    "        pedidos_base: Número base de pedidos diários\n",
    "\n",
    "    Returns:\n",
    "        DataFrame com dados de demanda\n",
    "    \"\"\"\n",
    "    print(\"Gerando dados sintéticos de demanda logística...\")\n",
    "\n",
    "    # Data inicial e final\n",
    "    data_inicio = pd.to_datetime(\"2024-01-01\")\n",
    "    datas = [data_inicio + timedelta(days=i) for i in range(dias)]\n",
    "\n",
    "    # Criar DataFrame base\n",
    "    dados = []\n",
    "\n",
    "    # Gerar dados diários\n",
    "    for data in datas:\n",
    "        # Verificar se é fim de semana (redução na demanda)\n",
    "        fator_dia_semana = 0.7 if data.weekday() >= 5 else 1.0\n",
    "\n",
    "        # Verificar eventos especiais\n",
    "        evento, fator_evento = verificar_evento_especial(data)\n",
    "\n",
    "        # Fator de sazonalidade mensal (maior demanda no fim do ano)\n",
    "        fator_mes = 1.0 + (data.month / 12) * 0.5\n",
    "\n",
    "        # Tendência de crescimento ao longo do ano\n",
    "        fator_tendencia = 1.0 + (data - data_inicio).days / (365 * 2)\n",
    "\n",
    "        # Para cada centro de distribuição\n",
    "        for cd_nome, cd_info in CDS.items():\n",
    "            # Obter informações de processos logísticos para este CD\n",
    "            processos_cd = PROCESSOS_LOGISTICOS_CD[cd_nome]\n",
    "\n",
    "            # Calcular demanda base para este CD (proporcional à capacidade)\n",
    "            demanda_base_cd = pedidos_base * (cd_info[\"capacidade_diaria\"] / 5000)\n",
    "\n",
    "            # Aplicar fatores\n",
    "            demanda_base = (\n",
    "                demanda_base_cd\n",
    "                * fator_dia_semana\n",
    "                * fator_evento\n",
    "                * fator_mes\n",
    "                * fator_tendencia\n",
    "            )\n",
    "\n",
    "            # Adicionar ruído aleatório (±15%)\n",
    "            ruido = np.random.uniform(0.85, 1.15)\n",
    "            demanda = int(demanda_base * ruido)\n",
    "\n",
    "            # Para cada categoria de produto\n",
    "            for categoria, cat_info in CATEGORIAS_PRODUTOS.items():\n",
    "                # Fator de sazonalidade por categoria\n",
    "                fator_sazonalidade = 1.0\n",
    "                if cat_info[\"sazonalidade\"] == \"alta\":\n",
    "                    # Produtos com alta sazonalidade são mais afetados por eventos e meses\n",
    "                    fator_sazonalidade = fator_evento * fator_mes\n",
    "                elif cat_info[\"sazonalidade\"] == \"media\":\n",
    "                    # Produtos com média sazonalidade são moderadamente afetados\n",
    "                    fator_sazonalidade = 1.0 + (fator_evento - 1.0) * 0.7\n",
    "\n",
    "                # Calcular proporção desta categoria no total (diferente por CD)\n",
    "                if cd_nome == \"Sao Paulo (SP)\":\n",
    "                    # São Paulo tem mais eletrônicos e vestuário\n",
    "                    proporcao = {\n",
    "                        \"Eletrônicos\": 0.25,\n",
    "                        \"Vestuário\": 0.20,\n",
    "                        \"Alimentos\": 0.10,\n",
    "                        \"Casa e Decoração\": 0.15,\n",
    "                        \"Beleza e Saúde\": 0.10,\n",
    "                        \"Esportes\": 0.08,\n",
    "                        \"Livros e Mídia\": 0.07,\n",
    "                        \"Brinquedos\": 0.05,\n",
    "                    }\n",
    "                elif cd_nome == \"Recife (PE)\" or cd_nome == \"Belem (PA)\":\n",
    "                    # Recife e Belém têm mais alimentos e beleza\n",
    "                    proporcao = {\n",
    "                        \"Eletrônicos\": 0.15,\n",
    "                        \"Vestuário\": 0.15,\n",
    "                        \"Alimentos\": 0.20,\n",
    "                        \"Casa e Decoração\": 0.10,\n",
    "                        \"Beleza e Saúde\": 0.15,\n",
    "                        \"Esportes\": 0.10,\n",
    "                        \"Livros e Mídia\": 0.05,\n",
    "                        \"Brinquedos\": 0.10,\n",
    "                    }\n",
    "                else:\n",
    "                    # Outros CDs têm distribuição mais equilibrada\n",
    "                    proporcao = {\n",
    "                        \"Eletrônicos\": 0.18,\n",
    "                        \"Vestuário\": 0.18,\n",
    "                        \"Alimentos\": 0.15,\n",
    "                        \"Casa e Decoração\": 0.12,\n",
    "                        \"Beleza e Saúde\": 0.12,\n",
    "                        \"Esportes\": 0.10,\n",
    "                        \"Livros e Mídia\": 0.08,\n",
    "                        \"Brinquedos\": 0.07,\n",
    "                    }\n",
    "\n",
    "                # Calcular demanda para esta categoria\n",
    "                demanda_categoria = int(\n",
    "                    demanda * proporcao[categoria] * fator_sazonalidade\n",
    "                )\n",
    "\n",
    "                # Adicionar ruído específico da categoria (±10%)\n",
    "                ruido_categoria = np.random.uniform(0.9, 1.1)\n",
    "                demanda_categoria = int(demanda_categoria * ruido_categoria)\n",
    "\n",
    "                # Calcular métricas derivadas\n",
    "                peso_total = demanda_categoria * cat_info[\"peso_medio\"]\n",
    "                volume_total = demanda_categoria * cat_info[\"volume_medio\"]\n",
    "                valor_total = demanda_categoria * cat_info[\"valor_medio\"]\n",
    "\n",
    "                # Calcular capacidade utilizada (%)\n",
    "                capacidade_utilizada = (\n",
    "                    demanda_categoria / cd_info[\"capacidade_diaria\"]\n",
    "                ) * 100\n",
    "\n",
    "                # Calcular lead time (dias) - varia por CD e categoria\n",
    "                lead_time_base = {\n",
    "                    \"Belem (PA)\": 3.5,\n",
    "                    \"Recife (PE)\": 3.0,\n",
    "                    \"Brasilia (DF)\": 2.5,\n",
    "                    \"Sao Paulo (SP)\": 1.5,\n",
    "                    \"Florianopolis (SC)\": 2.0,\n",
    "                }\n",
    "\n",
    "                # Ajuste de lead time por categoria\n",
    "                ajuste_lead_time = {\n",
    "                    \"Eletrônicos\": 1.2,\n",
    "                    \"Vestuário\": 1.0,\n",
    "                    \"Alimentos\": 0.8,\n",
    "                    \"Casa e Decoração\": 1.5,\n",
    "                    \"Beleza e Saúde\": 0.9,\n",
    "                    \"Esportes\": 1.1,\n",
    "                    \"Livros e Mídia\": 0.7,\n",
    "                    \"Brinquedos\": 1.0,\n",
    "                }\n",
    "\n",
    "                # Lead time final\n",
    "                lead_time = lead_time_base[cd_nome] * ajuste_lead_time[categoria]\n",
    "\n",
    "                # Adicionar ruído ao lead time (±20%)\n",
    "                lead_time = lead_time * np.random.uniform(0.8, 1.2)\n",
    "\n",
    "                # Calcular taxa de atraso (%) - maior quando a demanda está próxima da capacidade\n",
    "                taxa_atraso_base = (\n",
    "                    max(0, min(50, (capacidade_utilizada - 70) * 1.5))\n",
    "                    if capacidade_utilizada > 70\n",
    "                    else 5\n",
    "                )\n",
    "                taxa_atraso = taxa_atraso_base * np.random.uniform(0.8, 1.2)\n",
    "\n",
    "                # Calcular métricas de picking\n",
    "                # Tempo base de picking ajustado pela eficiência do CD e complexidade do produto\n",
    "                eficiencia_picking = processos_cd[\"eficiencia_picking\"]\n",
    "                complexidade_picking = 1.0\n",
    "                if cat_info[\"complexidade_picking\"] == \"alta\":\n",
    "                    complexidade_picking = 1.3\n",
    "                elif cat_info[\"complexidade_picking\"] == \"media\":\n",
    "                    complexidade_picking = 1.0\n",
    "                else:  # baixa\n",
    "                    complexidade_picking = 0.8\n",
    "\n",
    "                # Tempo de picking por item (minutos)\n",
    "                tempo_picking_item = (\n",
    "                    cat_info[\"tempo_base_picking\"]\n",
    "                    * complexidade_picking\n",
    "                    / eficiencia_picking\n",
    "                )\n",
    "\n",
    "                # Tempo total de picking para esta categoria/demanda\n",
    "                tempo_total_picking = tempo_picking_item * demanda_categoria\n",
    "\n",
    "                # Taxa de erro de picking - afetada pela complexidade e eficiência\n",
    "                taxa_erro_picking_base = processos_cd[\"taxa_erro_picking_base\"]\n",
    "                taxa_erro_picking = (\n",
    "                    taxa_erro_picking_base * complexidade_picking / eficiencia_picking\n",
    "                )\n",
    "\n",
    "                # Adicionar variação com base na ocupação do CD\n",
    "                if capacidade_utilizada > 85:\n",
    "                    # Mais erros quando o CD está muito ocupado\n",
    "                    taxa_erro_picking *= 1.5\n",
    "\n",
    "                # Calcular métricas de embalagem\n",
    "                # Tempo base de embalagem ajustado pela eficiência do CD e fragilidade do produto\n",
    "                eficiencia_embalagem = processos_cd[\"eficiencia_embalagem\"]\n",
    "                fragilidade = 1.0\n",
    "                if cat_info[\"fragilidade\"] == \"alta\":\n",
    "                    fragilidade = 1.4\n",
    "                elif cat_info[\"fragilidade\"] == \"media\":\n",
    "                    fragilidade = 1.1\n",
    "                else:  # baixa\n",
    "                    fragilidade = 0.9\n",
    "\n",
    "                # Tempo de embalagem por item (minutos)\n",
    "                tempo_embalagem_item = (\n",
    "                    cat_info[\"tempo_base_embalagem\"]\n",
    "                    * fragilidade\n",
    "                    / eficiencia_embalagem\n",
    "                )\n",
    "\n",
    "                # Tempo total de embalagem para esta categoria/demanda\n",
    "                tempo_total_embalagem = tempo_embalagem_item * demanda_categoria\n",
    "\n",
    "                # Taxa de erro de embalagem - afetada pela fragilidade e eficiência\n",
    "                taxa_erro_embalagem_base = processos_cd[\"taxa_erro_embalagem_base\"]\n",
    "                taxa_erro_embalagem = (\n",
    "                    taxa_erro_embalagem_base * fragilidade / eficiencia_embalagem\n",
    "                )\n",
    "\n",
    "                # Adicionar variação com base na ocupação do CD\n",
    "                if capacidade_utilizada > 85:\n",
    "                    # Mais erros quando o CD está muito ocupado\n",
    "                    taxa_erro_embalagem *= 1.4\n",
    "\n",
    "                # Calcular métricas de expedição\n",
    "                # Tempo de expedição ajustado pela eficiência do CD e prioridade do produto\n",
    "                eficiencia_expedicao = processos_cd[\"eficiencia_expedicao\"]\n",
    "                prioridade = 1.0\n",
    "                if cat_info[\"prioridade_expedicao\"] == \"alta\":\n",
    "                    prioridade = 0.8  # Produtos de alta prioridade são expedidos mais rapidamente\n",
    "                elif cat_info[\"prioridade_expedicao\"] == \"media\":\n",
    "                    prioridade = 1.0\n",
    "                else:  # baixa\n",
    "                    prioridade = 1.2\n",
    "\n",
    "                # Tempo de expedição por pedido (minutos)\n",
    "                tempo_expedicao = (\n",
    "                    processos_cd[\"tempo_medio_expedicao\"]\n",
    "                    * prioridade\n",
    "                    / eficiencia_expedicao\n",
    "                )\n",
    "\n",
    "                # Adicionar variação com base na ocupação do CD\n",
    "                if capacidade_utilizada > 90:\n",
    "                    # Expedição mais lenta quando o CD está muito ocupado\n",
    "                    tempo_expedicao *= 1.5\n",
    "\n",
    "                # Calcular gargalos nos processos\n",
    "                # Capacidade diária de picking (itens)\n",
    "                capacidade_picking_dia = (\n",
    "                    processos_cd[\"capacidade_picking_hora\"] * 16\n",
    "                )  # Considerando 16h de operação\n",
    "                gargalo_picking = (demanda_categoria / capacidade_picking_dia) * 100\n",
    "\n",
    "                # Capacidade diária de embalagem (itens)\n",
    "                capacidade_embalagem_dia = (\n",
    "                    processos_cd[\"capacidade_embalagem_hora\"] * 16\n",
    "                )\n",
    "                gargalo_embalagem = (demanda_categoria / capacidade_embalagem_dia) * 100\n",
    "\n",
    "                # Capacidade diária de expedição (pedidos)\n",
    "                capacidade_expedicao_dia = (\n",
    "                    processos_cd[\"capacidade_expedicao_hora\"] * 16\n",
    "                )\n",
    "                # Assumindo que cada pedido tem em média 3 itens\n",
    "                pedidos_estimados = demanda_categoria / 3\n",
    "                gargalo_expedicao = (pedidos_estimados / capacidade_expedicao_dia) * 100\n",
    "\n",
    "                # Adicionar registro ao dataset\n",
    "                dados.append(\n",
    "                    {\n",
    "                        \"data\": data,\n",
    "                        \"centro_distribuicao\": cd_nome,\n",
    "                        \"categoria_produto\": categoria,\n",
    "                        \"demanda\": demanda_categoria,\n",
    "                        \"peso_total_kg\": round(peso_total, 2),\n",
    "                        \"volume_total_m3\": round(volume_total, 3),\n",
    "                        \"valor_total\": round(valor_total, 2),\n",
    "                        \"capacidade_utilizada_pct\": round(capacidade_utilizada, 2),\n",
    "                        \"lead_time_dias\": round(lead_time, 2),\n",
    "                        \"taxa_atraso_pct\": round(taxa_atraso, 2),\n",
    "                        \"evento_especial\": evento,\n",
    "                        \"dia_semana\": data.day_name(),\n",
    "                        \"mes\": data.month_name(),\n",
    "                        \"trimestre\": f\"Q{(data.month-1)//3 + 1}\",\n",
    "                        \"tempo_picking_item_min\": round(tempo_picking_item, 2),\n",
    "                        \"tempo_total_picking_min\": round(tempo_total_picking, 2),\n",
    "                        \"taxa_erro_picking_pct\": round(taxa_erro_picking * 100, 2),\n",
    "                        \"gargalo_picking_pct\": round(gargalo_picking, 2),\n",
    "                        \"complexidade_picking\": cat_info[\"complexidade_picking\"],\n",
    "                        \"tempo_embalagem_item_min\": round(tempo_embalagem_item, 2),\n",
    "                        \"tempo_total_embalagem_min\": round(tempo_total_embalagem, 2),\n",
    "                        \"taxa_erro_embalagem_pct\": round(taxa_erro_embalagem * 100, 2),\n",
    "                        \"gargalo_embalagem_pct\": round(gargalo_embalagem, 2),\n",
    "                        \"fragilidade_produto\": cat_info[\"fragilidade\"],\n",
    "                        \"tempo_expedicao_min\": round(tempo_expedicao, 2),\n",
    "                        \"gargalo_expedicao_pct\": round(gargalo_expedicao, 2),\n",
    "                        \"prioridade_expedicao\": cat_info[\"prioridade_expedicao\"],\n",
    "                        \"nivel_automacao_cd\": processos_cd[\"nivel_automacao\"],\n",
    "                        \"tempo_total_processamento_min\": round(\n",
    "                            tempo_picking_item + tempo_embalagem_item + tempo_expedicao,\n",
    "                            2,\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Criar DataFrame\n",
    "    df = pd.DataFrame(dados)\n",
    "\n",
    "    # Adicionar features temporais\n",
    "    df[\"dia_do_mes\"] = df[\"data\"].dt.day\n",
    "    df[\"dia_do_ano\"] = df[\"data\"].dt.dayofyear\n",
    "    df[\"semana_do_ano\"] = df[\"data\"].dt.isocalendar().week\n",
    "    df[\"fim_de_semana\"] = df[\"data\"].dt.weekday >= 5\n",
    "\n",
    "    print(f\"Dados gerados com sucesso: {len(df)} registros.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45590d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Etapa 1 - Coleta e preparação de dados\n",
    "- Limpeza e preparação dos dados\n",
    "\"\"\"\n",
    "\n",
    "# Função para preparar os dados para modelagem\n",
    "def preparar_dados_modelagem(df):\n",
    "    \"\"\"\n",
    "    Prepara os dados para modelagem, incluindo engenharia de features,\n",
    "    tratamento de valores ausentes e normalização.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame com dados brutos\n",
    "\n",
    "    Returns:\n",
    "        DataFrame com dados preparados para modelagem\n",
    "    \"\"\"\n",
    "    print(\"Preparando dados para modelagem...\")\n",
    "\n",
    "    # Criar cópia para não modificar o original\n",
    "    df_prep = df.copy()\n",
    "\n",
    "    # Converter colunas categóricas para numéricas\n",
    "    df_prep[\"mes_num\"] = df_prep[\"data\"].dt.month\n",
    "    df_prep[\"dia_semana_num\"] = df_prep[\"data\"].dt.weekday\n",
    "\n",
    "    # Criar flag para eventos especiais (1 se houver evento, 0 caso contrário)\n",
    "    df_prep[\"tem_evento\"] = df_prep[\"evento_especial\"].notna().astype(int)\n",
    "\n",
    "    # Criar features de lag (demanda dos dias anteriores)\n",
    "    # Inicializar colunas de lag e médias móveis com NaN\n",
    "    for lag in [1, 7, 14]:\n",
    "        df_prep[f\"demanda_lag_{lag}\"] = np.nan\n",
    "    df_prep[\"demanda_ma7\"] = np.nan\n",
    "    df_prep[\"demanda_ma14\"] = np.nan\n",
    "\n",
    "    for cd in df_prep[\"centro_distribuicao\"].unique():\n",
    "        for categoria in df_prep[\"categoria_produto\"].unique():\n",
    "            # Filtrar dados para este CD e categoria\n",
    "            mask = (df_prep[\"centro_distribuicao\"] == cd) & (\n",
    "                df_prep[\"categoria_produto\"] == categoria\n",
    "            )\n",
    "\n",
    "            # Ordenar por data\n",
    "            temp = df_prep.loc[mask].sort_values(\"data\")\n",
    "\n",
    "            # Criar lags de 1, 7 e 14 dias\n",
    "            for lag in [1, 7, 14]:\n",
    "                col_name = f\"demanda_lag_{lag}\"\n",
    "                temp[col_name] = temp[\"demanda\"].shift(lag)\n",
    "\n",
    "            # Calcular média móvel de 7 e 14 dias\n",
    "            temp[\"demanda_ma7\"] = temp[\"demanda\"].rolling(window=7).mean()\n",
    "            temp[\"demanda_ma14\"] = temp[\"demanda\"].rolling(window=14).mean()\n",
    "\n",
    "            # Atualizar no DataFrame principal\n",
    "            df_prep.loc[\n",
    "                mask,\n",
    "                [f\"demanda_lag_{lag}\" for lag in [1, 7, 14]]\n",
    "                + [\"demanda_ma7\", \"demanda_ma14\"],\n",
    "            ] = temp[\n",
    "                [f\"demanda_lag_{lag}\" for lag in [1, 7, 14]]\n",
    "                + [\"demanda_ma7\", \"demanda_ma14\"]\n",
    "            ]\n",
    "\n",
    "    # Identificar colunas numéricas de lag e médias móveis\n",
    "    colunas_lag = [f\"demanda_lag_{lag}\" for lag in [1, 7, 14]] + [\n",
    "        \"demanda_ma7\",\n",
    "        \"demanda_ma14\",\n",
    "    ]\n",
    "\n",
    "    # Preencher valores NaN nas colunas de lag com a média\n",
    "    for col in colunas_lag:\n",
    "        if col in df_prep.columns and pd.api.types.is_numeric_dtype(df_prep[col]):\n",
    "            df_prep[col] = df_prep[col].fillna(df_prep[col].mean())\n",
    "        else:\n",
    "            print(\n",
    "                f\"Aviso: Coluna '{col}' não encontrada ou não numérica. Ignorando fillna.\"\n",
    "            )\n",
    "\n",
    "    # Criar features de tendência\n",
    "    df_prep[\"dias_desde_inicio\"] = (df_prep[\"data\"] - df_prep[\"data\"].min()).dt.days\n",
    "\n",
    "    # Criar features cíclicas para dia do ano e semana do ano\n",
    "    df_prep[\"dia_ano_sin\"] = np.sin(2 * np.pi * df_prep[\"dia_do_ano\"] / 365)\n",
    "    df_prep[\"dia_ano_cos\"] = np.cos(2 * np.pi * df_prep[\"dia_do_ano\"] / 365)\n",
    "    df_prep[\"semana_sin\"] = np.sin(\n",
    "        2 * np.pi * df_prep[\"semana_do_ano\"].astype(float) / 52\n",
    "    )\n",
    "    df_prep[\"semana_cos\"] = np.cos(\n",
    "        2 * np.pi * df_prep[\"semana_do_ano\"].astype(float) / 52\n",
    "    )\n",
    "\n",
    "    # Criar feature de proximidade com fim do mês (maior demanda próximo ao fim do mês)\n",
    "    df_prep[\"prox_fim_mes\"] = 1 - (df_prep[\"dia_do_mes\"] / 31)\n",
    "\n",
    "    # Codificar variáveis categóricas\n",
    "    # Complexidade de picking\n",
    "    df_prep[\"complexidade_picking_num\"] = df_prep[\"complexidade_picking\"].map(\n",
    "        {\"baixa\": 0, \"media\": 1, \"alta\": 2}\n",
    "    )\n",
    "\n",
    "    # Fragilidade do produto\n",
    "    df_prep[\"fragilidade_produto_num\"] = df_prep[\"fragilidade_produto\"].map(\n",
    "        {\"baixa\": 0, \"media\": 1, \"alta\": 2}\n",
    "    )\n",
    "\n",
    "    # Prioridade de expedição\n",
    "    df_prep[\"prioridade_expedicao_num\"] = df_prep[\"prioridade_expedicao\"].map(\n",
    "        {\"baixa\": 0, \"media\": 1, \"alta\": 2}\n",
    "    )\n",
    "\n",
    "    # Nível de automação do CD\n",
    "    df_prep[\"nivel_automacao_cd_num\"] = df_prep[\"nivel_automacao_cd\"].map(\n",
    "        {\"baixo\": 0, \"medio\": 1, \"alto\": 2}\n",
    "    )\n",
    "\n",
    "    # Criar features de gargalos combinados\n",
    "    df_prep[\"gargalo_combinado\"] = (\n",
    "        df_prep[\"gargalo_picking_pct\"]\n",
    "        + df_prep[\"gargalo_embalagem_pct\"]\n",
    "        + df_prep[\"gargalo_expedicao_pct\"]\n",
    "    ) / 3\n",
    "\n",
    "    # Criar feature de eficiência do processo\n",
    "    df_prep[\"eficiencia_processo\"] = (\n",
    "        100\n",
    "        - (df_prep[\"taxa_erro_picking_pct\"] + df_prep[\"taxa_erro_embalagem_pct\"]) / 2\n",
    "    )\n",
    "\n",
    "    print(\"Dados preparados com sucesso.\")\n",
    "    return df_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55c8c40",
   "metadata": {},
   "source": [
    "# Fim da Etapa 1 - Coleta e preparação de dados\n",
    "---\n",
    "## Relação de Dados Gerados \n",
    "\n",
    "| Nome da Coluna                  | Descrição                                                                 | Exemplo                  |\n",
    "|----------------------------------|--------------------------------------------------------------------------|--------------------------|\n",
    "| data                            | Data da operação                                                         | 2024-05-10               |\n",
    "| centro_distribuicao             | Nome do Centro de Distribuição                                           | Sao Paulo (SP)           |\n",
    "| categoria_produto               | Categoria do produto                                                     | Eletrônicos              |\n",
    "| demanda                         | Quantidade demandada para a categoria no CD                              | 320                      |\n",
    "| peso_total_kg                   | Peso total dos produtos demandados (kg)                                  | 1120.50                  |\n",
    "| volume_total_m3                 | Volume total dos produtos demandados (m³)                                | 4.800                    |\n",
    "| valor_total                     | Valor total dos produtos demandados (R$)                                 | 384000.00                |\n",
    "| capacidade_utilizada_pct        | Percentual da capacidade diária do CD utilizada                          | 64.00                    |\n",
    "| lead_time_dias                  | Tempo médio de atendimento do pedido (dias)                              | 1.80                     |\n",
    "| taxa_atraso_pct                 | Percentual de pedidos com atraso                                         | 7.20                     |\n",
    "| evento_especial                 | Evento especial que impactou a demanda                                   | Black Friday             |\n",
    "| dia_semana                      | Dia da semana                                                            | Friday                   |\n",
    "| mes                             | Nome do mês                                                              | May                      |\n",
    "| trimestre                       | Trimestre do ano                                                         | Q2                       |\n",
    "| tempo_picking_item_min          | Tempo médio de picking por item (minutos)                                | 5.20                     |\n",
    "| tempo_total_picking_min         | Tempo total de picking para a categoria (minutos)                        | 1664.00                  |\n",
    "| taxa_erro_picking_pct           | Percentual de erros no picking                                           | 4.50                     |\n",
    "| gargalo_picking_pct             | Percentual de ocupação da capacidade de picking                          | 45.00                    |\n",
    "| complexidade_picking            | Complexidade do picking para a categoria                                 | alta                     |\n",
    "| tempo_embalagem_item_min        | Tempo médio de embalagem por item (minutos)                              | 4.90                     |\n",
    "| tempo_total_embalagem_min       | Tempo total de embalagem para a categoria (minutos)                      | 1568.00                  |\n",
    "| taxa_erro_embalagem_pct         | Percentual de erros na embalagem                                         | 2.80                     |\n",
    "| gargalo_embalagem_pct           | Percentual de ocupação da capacidade de embalagem                        | 39.00                    |\n",
    "| fragilidade_produto             | Fragilidade do produto                                                   | alta                     |\n",
    "| tempo_expedicao_min             | Tempo médio de expedição por pedido (minutos)                            | 75.00                    |\n",
    "| gargalo_expedicao_pct           | Percentual de ocupação da capacidade de expedição                        | 30.00                    |\n",
    "| prioridade_expedicao            | Prioridade de expedição da categoria                                     | media                    |\n",
    "| nivel_automacao_cd              | Nível de automação do CD                                                 | alto                     |\n",
    "| tempo_total_processamento_min   | Tempo total de processamento (picking + embalagem + expedição, minutos)  | 185.10                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e81950",
   "metadata": {},
   "source": [
    "# Início da Etapa 2 - Análise Exploratória de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1491b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Etapa 2 - Análise exploratória e visualização\n",
    "- Análise de correlação\n",
    "- Visualização de dados\n",
    "- Identificação de padrões e tendências\n",
    "- Insights\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Função para realizar análise exploratória dos dados\n",
    "def analise_exploratoria(df):\n",
    "    \"\"\"\n",
    "    Realiza análise exploratória dos dados, gerando visualizações e insights.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame com dados de demanda\n",
    "\n",
    "    Returns:\n",
    "        Dicionário com insights da análise\n",
    "    \"\"\"\n",
    "    print(\"Realizando análise exploratória dos dados...\")\n",
    "\n",
    "    # Criar diretório para visualizações\n",
    "    os.makedirs(\"output/visualizacoes\", exist_ok=True)\n",
    "\n",
    "    # Resumo estatístico\n",
    "    print(\"\\nResumo estatístico dos dados:\")\n",
    "    display(df.describe())\n",
    "\n",
    "    # Verificar valores ausentes\n",
    "    print(\"\\nValores ausentes por coluna:\")\n",
    "    display(df.isna().sum())\n",
    "\n",
    "    # Análise temporal da demanda\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    demanda_diaria = df.groupby(\"data\")[\"demanda\"].sum().reset_index()\n",
    "    plt.plot(demanda_diaria[\"data\"], demanda_diaria[\"demanda\"])\n",
    "    plt.title(\"Demanda Total Diária\")\n",
    "    plt.xlabel(\"Data\")\n",
    "    plt.ylabel(\"Demanda (unidades)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/demanda_diaria.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Demanda por centro de distribuição\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    demanda_cd = (\n",
    "        df.groupby([\"data\", \"centro_distribuicao\"])[\"demanda\"].sum().reset_index()\n",
    "    )\n",
    "    for cd in demanda_cd[\"centro_distribuicao\"].unique():\n",
    "        data_cd = demanda_cd[demanda_cd[\"centro_distribuicao\"] == cd]\n",
    "        plt.plot(data_cd[\"data\"], data_cd[\"demanda\"], label=cd)\n",
    "    plt.title(\"Demanda Diária por Centro de Distribuição\")\n",
    "    plt.xlabel(\"Data\")\n",
    "    plt.ylabel(\"Demanda (unidades)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/demanda_por_cd.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Demanda por categoria de produto\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    demanda_categoria = (\n",
    "        df.groupby([\"data\", \"categoria_produto\"])[\"demanda\"].sum().reset_index()\n",
    "    )\n",
    "    for categoria in demanda_categoria[\"categoria_produto\"].unique():\n",
    "        data_cat = demanda_categoria[\n",
    "            demanda_categoria[\"categoria_produto\"] == categoria\n",
    "        ]\n",
    "        plt.plot(data_cat[\"data\"], data_cat[\"demanda\"], label=categoria)\n",
    "    plt.title(\"Demanda Diária por Categoria de Produto\")\n",
    "    plt.xlabel(\"Data\")\n",
    "    plt.ylabel(\"Demanda (unidades)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/demanda_por_categoria.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Distribuição da demanda\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df[\"demanda\"], kde=True)\n",
    "    plt.title(\"Distribuição da Demanda\")\n",
    "    plt.xlabel(\"Demanda (unidades)\")\n",
    "    plt.ylabel(\"Frequência\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/distribuicao_demanda.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Boxplot da demanda por centro de distribuição\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x=\"centro_distribuicao\", y=\"demanda\", data=df)\n",
    "    plt.title(\"Distribuição da Demanda por Centro de Distribuição\")\n",
    "    plt.xlabel(\"Centro de Distribuição\")\n",
    "    plt.ylabel(\"Demanda (unidades)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/boxplot_demanda_cd.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Boxplot da demanda por categoria de produto\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x=\"categoria_produto\", y=\"demanda\", data=df)\n",
    "    plt.title(\"Distribuição da Demanda por Categoria de Produto\")\n",
    "    plt.xlabel(\"Categoria de Produto\")\n",
    "    plt.ylabel(\"Demanda (unidades)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/boxplot_demanda_categoria.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Heatmap da demanda por dia da semana e mês\n",
    "    demanda_dia_mes = df.groupby([\"dia_semana\", \"mes\"])[\"demanda\"].mean().reset_index()\n",
    "    demanda_dia_mes_pivot = demanda_dia_mes.pivot(\n",
    "        index=\"dia_semana\", columns=\"mes\", values=\"demanda\"\n",
    "    )\n",
    "\n",
    "    # Reordenar dias da semana e meses\n",
    "    dias_ordem = [\n",
    "        \"Monday\",\n",
    "        \"Tuesday\",\n",
    "        \"Wednesday\",\n",
    "        \"Thursday\",\n",
    "        \"Friday\",\n",
    "        \"Saturday\",\n",
    "        \"Sunday\",\n",
    "    ]\n",
    "    meses_ordem = [\n",
    "        \"January\",\n",
    "        \"February\",\n",
    "        \"March\",\n",
    "        \"April\",\n",
    "        \"May\",\n",
    "        \"June\",\n",
    "        \"July\",\n",
    "        \"August\",\n",
    "        \"September\",\n",
    "        \"October\",\n",
    "        \"November\",\n",
    "        \"December\",\n",
    "    ]\n",
    "\n",
    "    demanda_dia_mes_pivot = demanda_dia_mes_pivot.reindex(dias_ordem)\n",
    "    demanda_dia_mes_pivot = demanda_dia_mes_pivot.reindex(columns=meses_ordem)\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(demanda_dia_mes_pivot, annot=True, fmt=\".0f\", cmap=\"YlGnBu\")\n",
    "    plt.title(\"Demanda Média por Dia da Semana e Mês\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/heatmap_demanda_dia_mes.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Impacto dos eventos especiais na demanda\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    demanda_evento = df.groupby(\"evento_especial\")[\"demanda\"].mean().reset_index()\n",
    "    demanda_evento = demanda_evento.sort_values(\"demanda\", ascending=False)\n",
    "    sns.barplot(x=\"evento_especial\", y=\"demanda\", data=demanda_evento)\n",
    "    plt.title(\"Demanda Média por Evento Especial\")\n",
    "    plt.xlabel(\"Evento Especial\")\n",
    "    plt.ylabel(\"Demanda Média (unidades)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/demanda_por_evento.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Correlação entre variáveis\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    colunas_numericas = df.select_dtypes(include=[np.number]).columns\n",
    "    corr = df[colunas_numericas].corr()\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "    plt.title(\"Matriz de Correlação\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/matriz_correlacao.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Decomposição da série temporal\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    demanda_diaria = df.groupby(\"data\")[\"demanda\"].sum().reset_index()\n",
    "    demanda_diaria.set_index(\"data\", inplace=True)\n",
    "\n",
    "    # Decomposição da série temporal\n",
    "    decomposicao = seasonal_decompose(demanda_diaria, model=\"additive\", period=7)\n",
    "\n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.plot(demanda_diaria)\n",
    "    plt.title(\"Demanda Original\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.plot(decomposicao.trend)\n",
    "    plt.title(\"Tendência\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(4, 1, 3)\n",
    "    plt.plot(decomposicao.seasonal)\n",
    "    plt.title(\"Sazonalidade\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(4, 1, 4)\n",
    "    plt.plot(decomposicao.resid)\n",
    "    plt.title(\"Resíduos\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/decomposicao_serie_temporal.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Análise de capacidade utilizada\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    capacidade_cd = (\n",
    "        df.groupby([\"data\", \"centro_distribuicao\"])[\"capacidade_utilizada_pct\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    for cd in capacidade_cd[\"centro_distribuicao\"].unique():\n",
    "        data_cd = capacidade_cd[capacidade_cd[\"centro_distribuicao\"] == cd]\n",
    "        plt.plot(data_cd[\"data\"], data_cd[\"capacidade_utilizada_pct\"], label=cd)\n",
    "    plt.title(\"Capacidade Utilizada por Centro de Distribuição\")\n",
    "    plt.xlabel(\"Data\")\n",
    "    plt.ylabel(\"Capacidade Utilizada (%)\")\n",
    "    plt.axhline(y=80, color=\"r\", linestyle=\"--\", label=\"Limite de Alerta (80%)\")\n",
    "    plt.axhline(y=100, color=\"darkred\", linestyle=\"--\", label=\"Capacidade Máxima\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/capacidade_utilizada.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Relação entre capacidade utilizada e taxa de atraso\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(\n",
    "        x=\"capacidade_utilizada_pct\",\n",
    "        y=\"taxa_atraso_pct\",\n",
    "        hue=\"centro_distribuicao\",\n",
    "        data=df,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    plt.title(\"Relação entre Capacidade Utilizada e Taxa de Atraso\")\n",
    "    plt.xlabel(\"Capacidade Utilizada (%)\")\n",
    "    plt.ylabel(\"Taxa de Atraso (%)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/capacidade_vs_atraso.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Análise dos processos de picking, embalagem e expedição\n",
    "    # Tempo médio de picking por categoria de produto\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    tempo_picking = (\n",
    "        df.groupby(\"categoria_produto\")[\"tempo_picking_item_min\"].mean().reset_index()\n",
    "    )\n",
    "    tempo_picking = tempo_picking.sort_values(\"tempo_picking_item_min\", ascending=False)\n",
    "    sns.barplot(x=\"categoria_produto\", y=\"tempo_picking_item_min\", data=tempo_picking)\n",
    "    plt.title(\"Tempo Médio de Picking por Categoria de Produto\")\n",
    "    plt.xlabel(\"Categoria de Produto\")\n",
    "    plt.ylabel(\"Tempo (minutos)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/tempo_picking_categoria.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Tempo médio de embalagem por categoria de produto\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    tempo_embalagem = (\n",
    "        df.groupby(\"categoria_produto\")[\"tempo_embalagem_item_min\"].mean().reset_index()\n",
    "    )\n",
    "    tempo_embalagem = tempo_embalagem.sort_values(\n",
    "        \"tempo_embalagem_item_min\", ascending=False\n",
    "    )\n",
    "    sns.barplot(\n",
    "        x=\"categoria_produto\", y=\"tempo_embalagem_item_min\", data=tempo_embalagem\n",
    "    )\n",
    "    plt.title(\"Tempo Médio de Embalagem por Categoria de Produto\")\n",
    "    plt.xlabel(\"Categoria de Produto\")\n",
    "    plt.ylabel(\"Tempo (minutos)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/tempo_embalagem_categoria.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Tempo médio de expedição por centro de distribuição\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    tempo_expedicao = (\n",
    "        df.groupby(\"centro_distribuicao\")[\"tempo_expedicao_min\"].mean().reset_index()\n",
    "    )\n",
    "    tempo_expedicao = tempo_expedicao.sort_values(\n",
    "        \"tempo_expedicao_min\", ascending=False\n",
    "    )\n",
    "    sns.barplot(x=\"centro_distribuicao\", y=\"tempo_expedicao_min\", data=tempo_expedicao)\n",
    "    plt.title(\"Tempo Médio de Expedição por Centro de Distribuição\")\n",
    "    plt.xlabel(\"Centro de Distribuição\")\n",
    "    plt.ylabel(\"Tempo (minutos)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/tempo_expedicao_cd.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Taxa de erro de picking por centro de distribuição\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    erro_picking = (\n",
    "        df.groupby(\"centro_distribuicao\")[\"taxa_erro_picking_pct\"].mean().reset_index()\n",
    "    )\n",
    "    erro_picking = erro_picking.sort_values(\"taxa_erro_picking_pct\", ascending=False)\n",
    "    sns.barplot(x=\"centro_distribuicao\", y=\"taxa_erro_picking_pct\", data=erro_picking)\n",
    "    plt.title(\"Taxa de Erro de Picking por Centro de Distribuição\")\n",
    "    plt.xlabel(\"Centro de Distribuição\")\n",
    "    plt.ylabel(\"Taxa de Erro (%)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/erro_picking_cd.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Taxa de erro de embalagem por centro de distribuição\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    erro_embalagem = (\n",
    "        df.groupby(\"centro_distribuicao\")[\"taxa_erro_embalagem_pct\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    erro_embalagem = erro_embalagem.sort_values(\n",
    "        \"taxa_erro_embalagem_pct\", ascending=False\n",
    "    )\n",
    "    sns.barplot(\n",
    "        x=\"centro_distribuicao\", y=\"taxa_erro_embalagem_pct\", data=erro_embalagem\n",
    "    )\n",
    "    plt.title(\"Taxa de Erro de Embalagem por Centro de Distribuição\")\n",
    "    plt.xlabel(\"Centro de Distribuição\")\n",
    "    plt.ylabel(\"Taxa de Erro (%)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/erro_embalagem_cd.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Análise de gargalos nos processos\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    gargalos = (\n",
    "        df.groupby([\"data\", \"centro_distribuicao\"])[\n",
    "            [\"gargalo_picking_pct\", \"gargalo_embalagem_pct\", \"gargalo_expedicao_pct\"]\n",
    "        ]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Criar um gráfico para cada tipo de gargalo\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    for cd in gargalos[\"centro_distribuicao\"].unique():\n",
    "        data_cd = gargalos[gargalos[\"centro_distribuicao\"] == cd]\n",
    "        plt.plot(\n",
    "            data_cd[\"data\"], data_cd[\"gargalo_picking_pct\"], label=f\"{cd} - Picking\"\n",
    "        )\n",
    "    plt.title(\"Gargalos de Picking por Centro de Distribuição\")\n",
    "    plt.xlabel(\"Data\")\n",
    "    plt.ylabel(\"Utilização da Capacidade (%)\")\n",
    "    plt.axhline(y=80, color=\"r\", linestyle=\"--\", label=\"Limite de Alerta (80%)\")\n",
    "    plt.axhline(y=100, color=\"darkred\", linestyle=\"--\", label=\"Capacidade Máxima\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/gargalos_picking.png\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    for cd in gargalos[\"centro_distribuicao\"].unique():\n",
    "        data_cd = gargalos[gargalos[\"centro_distribuicao\"] == cd]\n",
    "        plt.plot(\n",
    "            data_cd[\"data\"], data_cd[\"gargalo_embalagem_pct\"], label=f\"{cd} - Embalagem\"\n",
    "        )\n",
    "    plt.title(\"Gargalos de Embalagem por Centro de Distribuição\")\n",
    "    plt.xlabel(\"Data\")\n",
    "    plt.ylabel(\"Utilização da Capacidade (%)\")\n",
    "    plt.axhline(y=80, color=\"r\", linestyle=\"--\", label=\"Limite de Alerta (80%)\")\n",
    "    plt.axhline(y=100, color=\"darkred\", linestyle=\"--\", label=\"Capacidade Máxima\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/gargalos_embalagem.png\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    for cd in gargalos[\"centro_distribuicao\"].unique():\n",
    "        data_cd = gargalos[gargalos[\"centro_distribuicao\"] == cd]\n",
    "        plt.plot(\n",
    "            data_cd[\"data\"], data_cd[\"gargalo_expedicao_pct\"], label=f\"{cd} - Expedição\"\n",
    "        )\n",
    "    plt.title(\"Gargalos de Expedição por Centro de Distribuição\")\n",
    "    plt.xlabel(\"Data\")\n",
    "    plt.ylabel(\"Utilização da Capacidade (%)\")\n",
    "    plt.axhline(y=80, color=\"r\", linestyle=\"--\", label=\"Limite de Alerta (80%)\")\n",
    "    plt.axhline(y=100, color=\"darkred\", linestyle=\"--\", label=\"Capacidade Máxima\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/gargalos_expedicao.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Comparação dos tempos de processamento\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    tempos_processamento = (\n",
    "        df.groupby(\"centro_distribuicao\")[\n",
    "            [\n",
    "                \"tempo_picking_item_min\",\n",
    "                \"tempo_embalagem_item_min\",\n",
    "                \"tempo_expedicao_min\",\n",
    "            ]\n",
    "        ]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Converter para formato longo para facilitar a visualização\n",
    "    tempos_processamento_long = pd.melt(\n",
    "        tempos_processamento,\n",
    "        id_vars=[\"centro_distribuicao\"],\n",
    "        value_vars=[\n",
    "            \"tempo_picking_item_min\",\n",
    "            \"tempo_embalagem_item_min\",\n",
    "            \"tempo_expedicao_min\",\n",
    "        ],\n",
    "        var_name=\"Processo\",\n",
    "        value_name=\"Tempo (min)\",\n",
    "    )\n",
    "\n",
    "    # Mapear nomes mais legíveis para os processos\n",
    "    tempos_processamento_long[\"Processo\"] = tempos_processamento_long[\"Processo\"].map(\n",
    "        {\n",
    "            \"tempo_picking_item_min\": \"Picking\",\n",
    "            \"tempo_embalagem_item_min\": \"Embalagem\",\n",
    "            \"tempo_expedicao_min\": \"Expedição\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(\n",
    "        x=\"centro_distribuicao\",\n",
    "        y=\"Tempo (min)\",\n",
    "        hue=\"Processo\",\n",
    "        data=tempos_processamento_long,\n",
    "    )\n",
    "    plt.title(\"Comparação dos Tempos de Processamento por Centro de Distribuição\")\n",
    "    plt.xlabel(\"Centro de Distribuição\")\n",
    "    plt.ylabel(\"Tempo Médio (minutos)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/visualizacoes/comparacao_tempos_processamento.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Insights da análise\n",
    "    insights = {\n",
    "        \"padroes_temporais\": \"Identificamos claros padrões sazonais na demanda, com picos durante eventos especiais como Black Friday e Natal. Também há variações semanais, com menor demanda nos fins de semana.\",\n",
    "        \"diferencas_cds\": \"O CD de São Paulo apresenta a maior demanda e utilização de capacidade, seguido por Brasília. Belém e Florianópolis têm demanda mais baixa e estável.\",\n",
    "        \"categorias_produtos\": \"Eletrônicos e Vestuário são as categorias com maior demanda e maior variabilidade sazonal, enquanto Alimentos apresenta demanda mais estável ao longo do ano.\",\n",
    "        \"gargalos_capacidade\": \"Identificamos períodos críticos onde a capacidade utilizada ultrapassa 80%, especialmente em São Paulo durante eventos especiais, o que está diretamente correlacionado com aumento nas taxas de atraso.\",\n",
    "        \"fatores_influencia\": \"Os principais fatores que influenciam a demanda são: eventos especiais (impacto de até 150%), sazonalidade mensal (maior no fim do ano), dia da semana (menor nos fins de semana) e categoria do produto.\",\n",
    "        \"gargalos_processos\": \"O processo de picking é o principal gargalo em todos os CDs, especialmente para produtos de alta complexidade como Eletrônicos e Casa e Decoração. A expedição apresenta gargalos significativos durante eventos especiais.\",\n",
    "        \"erros_processos\": \"As taxas de erro de picking e embalagem aumentam significativamente quando a capacidade utilizada ultrapassa 85%, especialmente em CDs com baixo nível de automação como Belém e Florianópolis.\",\n",
    "        \"eficiencia_processos\": \"São Paulo apresenta a maior eficiência em todos os processos devido ao alto nível de automação, enquanto Belém apresenta os maiores tempos de processamento e taxas de erro.\",\n",
    "    }\n",
    "\n",
    "    print(\"Análise exploratória concluída com sucesso.\")\n",
    "    return insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d14ca05",
   "metadata": {},
   "source": [
    "# Fim da Etapa 2 - Análise Exploratória de Dados\n",
    "---\n",
    "## Insights e Conclusões da análise Exploratória\n",
    "\n",
    "### Principais insights identificados:\n",
    "\n",
    "- Padrões temporais: Identificamos claros padrões sazonais na demanda, com picos durante eventos especiais como Black Friday e Natal. Também há variações semanais, com menor demanda nos fins de semana.\n",
    "- Diferenças entre CDs: O CD de São Paulo apresenta a maior demanda e utilização de capacidade, seguido por Brasília. Belém e Florianópolis têm demanda mais baixa e estável.\n",
    "- Categorias de produtos: Eletrônicos e Vestuário são as categorias com maior demanda e maior variabilidade sazonal, enquanto Alimentos apresenta demanda mais estável ao longo do ano.\n",
    "- Gargalos de capacidade: Identificamos períodos críticos onde a capacidade utilizada ultrapassa 80%, especialmente em São Paulo durante eventos especiais, o que está diretamente correlacionado com aumento nas taxas de atraso.\n",
    "- Fatores de influência: Os principais fatores que influenciam a demanda são: eventos especiais (impacto de até 150%), sazonalidade mensal (maior no fim do ano), dia da semana (menor nos fins de semana) e categoria do produto.\n",
    "- Gargalos nos processos: O processo de picking é o principal gargalo em todos os CDs, especialmente para produtos de alta complexidade como Eletrônicos e Casa e Decoração. A expedição apresenta gargalos significativos durante eventos especiais.\n",
    "- Erros nos processos: As taxas de erro de picking e embalagem aumentam significativamente quando a capacidade utilizada ultrapassa 85%, especialmente em CDs com baixo nível de automação como Belém e Florianópolis.\n",
    "- Eficiência dos processos: São Paulo apresenta a maior eficiência em todos os processos devido ao alto nível de automação, enquanto Belém apresenta os maiores tempos de processamento e taxas de erro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65aeddb",
   "metadata": {},
   "source": [
    "# Início da Etapa 3 - Desenvolvimento dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0b8acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Etapa 3 - Desenvolvimento dos modelos\n",
    "- treinamento de modelos\n",
    "\"\"\"\n",
    "\n",
    "# Função para treinar modelos de previsão de demanda\n",
    "def treinar_modelos_previsao(df, horizonte_previsao=30):\n",
    "    \"\"\"\n",
    "    Treina diferentes modelos de machine learning para previsão de demanda.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame com dados preparados\n",
    "        horizonte_previsao: Número de dias para prever no futuro\n",
    "\n",
    "    Returns:\n",
    "        Dicionário com modelos treinados e métricas de avaliação\n",
    "    \"\"\"\n",
    "    print(\"Treinando modelos de previsão de demanda...\")\n",
    "\n",
    "    # Criar diretório para modelos\n",
    "    os.makedirs('output/modelos', exist_ok=True)\n",
    "\n",
    "    # Preparar features e target\n",
    "    features = [\n",
    "        'mes_num', 'dia_semana_num', 'tem_evento',\n",
    "        'demanda_lag_1', 'demanda_lag_7', 'demanda_lag_14',\n",
    "        'demanda_ma7', 'demanda_ma14', 'dias_desde_inicio',\n",
    "        'dia_ano_sin', 'dia_ano_cos', 'semana_sin', 'semana_cos',\n",
    "        'prox_fim_mes', 'capacidade_utilizada_pct',\n",
    "        'complexidade_picking_num', 'fragilidade_produto_num',\n",
    "        'prioridade_expedicao_num', 'nivel_automacao_cd_num',\n",
    "        'tempo_picking_item_min', 'tempo_embalagem_item_min',\n",
    "        'tempo_expedicao_min', 'gargalo_picking_pct',\n",
    "        'gargalo_embalagem_pct', 'gargalo_expedicao_pct'\n",
    "    ]\n",
    "\n",
    "    target = 'demanda'\n",
    "\n",
    "    # Criar dicionário para armazenar resultados\n",
    "    resultados = {}\n",
    "\n",
    "    # Para cada combinação de CD e categoria, treinar modelos específicos\n",
    "    for cd in df['centro_distribuicao'].unique():\n",
    "        resultados[cd] = {}\n",
    "\n",
    "        for categoria in df['categoria_produto'].unique():\n",
    "            print(f\"Treinando modelos para {cd} - {categoria}...\")\n",
    "\n",
    "            # Filtrar dados para este CD e categoria\n",
    "            df_filtrado = df[(df['centro_distribuicao'] == cd) &\n",
    "                            (df['categoria_produto'] == categoria)].copy()\n",
    "\n",
    "            # Ordenar por data\n",
    "            df_filtrado = df_filtrado.sort_values('data')\n",
    "\n",
    "            # Dividir em treino e teste (80% treino, 20% teste)\n",
    "            # Usar divisão temporal para evitar data leakage\n",
    "            data_corte = df_filtrado['data'].max() - pd.Timedelta(days=horizonte_previsao)\n",
    "            df_treino = df_filtrado[df_filtrado['data'] <= data_corte]\n",
    "            df_teste = df_filtrado[df_filtrado['data'] > data_corte]\n",
    "\n",
    "            # Verificar se há dados suficientes\n",
    "            if len(df_treino) < 30 or len(df_teste) < 7:\n",
    "                print(f\"Dados insuficientes para {cd} - {categoria}. Pulando...\")\n",
    "                continue\n",
    "\n",
    "            # Preparar conjuntos de treino e teste\n",
    "            X_treino = df_treino[features]\n",
    "            y_treino = df_treino[target]\n",
    "            X_teste = df_teste[features]\n",
    "            y_teste = df_teste[target]\n",
    "\n",
    "            # Inicializar modelos\n",
    "            modelos = {\n",
    "                'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "                'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "                'XGBoost': XGBRegressor(n_estimators=100, random_state=42),\n",
    "                'LinearRegression': LinearRegression()\n",
    "            }\n",
    "\n",
    "            # Treinar e avaliar cada modelo\n",
    "            metricas_modelos = {}\n",
    "\n",
    "            for nome_modelo, modelo in modelos.items():\n",
    "                # Treinar modelo\n",
    "                modelo.fit(X_treino, y_treino)\n",
    "\n",
    "                # Fazer previsões\n",
    "                y_pred_treino = modelo.predict(X_treino)\n",
    "                y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "                # Calcular métricas\n",
    "                metricas_treino = {\n",
    "                    'MAE': mean_absolute_error(y_treino, y_pred_treino),\n",
    "                    'RMSE': np.sqrt(mean_squared_error(y_treino, y_pred_treino)),\n",
    "                    'R2': r2_score(y_treino, y_pred_treino)\n",
    "                }\n",
    "\n",
    "                metricas_teste = {\n",
    "                    'MAE': mean_absolute_error(y_teste, y_pred_teste),\n",
    "                    'RMSE': np.sqrt(mean_squared_error(y_teste, y_pred_teste)),\n",
    "                    'R2': r2_score(y_teste, y_pred_teste)\n",
    "                }\n",
    "\n",
    "                # Armazenar resultados\n",
    "                metricas_modelos[nome_modelo] = {\n",
    "                    'modelo': modelo,\n",
    "                    'metricas_treino': metricas_treino,\n",
    "                    'metricas_teste': metricas_teste,\n",
    "                    'importancia_features': None\n",
    "                }\n",
    "\n",
    "                # Armazenar importância das features (se disponível)\n",
    "                if hasattr(modelo, 'feature_importances_'):\n",
    "                    importancia = dict(zip(features, modelo.feature_importances_))\n",
    "                    metricas_modelos[nome_modelo]['importancia_features'] = importancia\n",
    "\n",
    "            # Identificar melhor modelo\n",
    "            melhor_modelo = min(metricas_modelos.items(),\n",
    "                               key=lambda x: x[1]['metricas_teste']['RMSE'])\n",
    "\n",
    "            # Armazenar resultados para este CD e categoria\n",
    "            resultados[cd][categoria] = {\n",
    "                'modelos': metricas_modelos,\n",
    "                'melhor_modelo': melhor_modelo[0],\n",
    "                'X_treino': X_treino,\n",
    "                'y_treino': y_treino,\n",
    "                'X_teste': X_teste,\n",
    "                'y_teste': y_teste\n",
    "            }\n",
    "\n",
    "            # Visualizar resultados do melhor modelo\n",
    "            melhor_nome = melhor_modelo[0]\n",
    "            melhor_modelo_obj = metricas_modelos[melhor_nome]['modelo']\n",
    "\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(df_teste['data'], y_teste, label='Real', marker='o')\n",
    "            plt.plot(df_teste['data'], melhor_modelo_obj.predict(X_teste),\n",
    "                    label=f'Previsão ({melhor_nome})', marker='x')\n",
    "            plt.title(f'Previsão de Demanda: {cd} - {categoria}')\n",
    "            plt.xlabel('Data')\n",
    "            plt.ylabel('Demanda')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'output/modelos/previsao_{cd}_{categoria}.png'.replace(' ', '_'))\n",
    "            plt.close()\n",
    "\n",
    "            # Se o modelo tem importância de features, visualizar\n",
    "            if metricas_modelos[melhor_nome]['importancia_features'] is not None:\n",
    "                importancia = metricas_modelos[melhor_nome]['importancia_features']\n",
    "                importancia_sorted = dict(sorted(importancia.items(),\n",
    "                                               key=lambda x: x[1], reverse=True))\n",
    "\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.bar(importancia_sorted.keys(), importancia_sorted.values())\n",
    "                plt.title(f'Importância das Features: {cd} - {categoria} ({melhor_nome})')\n",
    "                plt.xlabel('Feature')\n",
    "                plt.ylabel('Importância')\n",
    "                plt.xticks(rotation=90)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'output/modelos/importancia_{cd}_{categoria}.png'.replace(' ', '_'))\n",
    "                plt.close()\n",
    "\n",
    "    print(\"Treinamento de modelos concluído com sucesso.\")\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6400f4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Etapa 3 - Desenvolvimento dos modelos\n",
    "- Geração de previsões de demandas\n",
    "\"\"\"\n",
    "# Função para gerar previsões futuras\n",
    "def gerar_previsoes_futuras(df, resultados_modelos, dias_futuros=30):\n",
    "    \"\"\"\n",
    "    Gera previsões de demanda para os próximos dias.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame com dados históricos\n",
    "        resultados_modelos: Dicionário com modelos treinados\n",
    "        dias_futuros: Número de dias para prever\n",
    "\n",
    "    Returns:\n",
    "        DataFrame com previsões futuras\n",
    "    \"\"\"\n",
    "    print(f\"Gerando previsões para os próximos {dias_futuros} dias...\")\n",
    "\n",
    "    # Data máxima nos dados históricos\n",
    "    data_max = df['data'].max()\n",
    "\n",
    "    # Criar datas futuras\n",
    "    datas_futuras = [data_max + timedelta(days=i+1) for i in range(dias_futuros)]\n",
    "\n",
    "    # Lista para armazenar previsões\n",
    "    previsoes = []\n",
    "\n",
    "    # Para cada CD e categoria\n",
    "    for cd in resultados_modelos.keys():\n",
    "        for categoria in resultados_modelos[cd].keys():\n",
    "            # Obter o melhor modelo\n",
    "            melhor_modelo_nome = resultados_modelos[cd][categoria]['melhor_modelo']\n",
    "            melhor_modelo = resultados_modelos[cd][categoria]['modelos'][melhor_modelo_nome]['modelo']\n",
    "\n",
    "            # Filtrar dados históricos para este CD e categoria\n",
    "            df_hist = df[(df['centro_distribuicao'] == cd) &\n",
    "                         (df['categoria_produto'] == categoria)].copy()\n",
    "\n",
    "            # Ordenar por data\n",
    "            df_hist = df_hist.sort_values('data')\n",
    "\n",
    "            # Obter informações de processos logísticos para este CD\n",
    "            processos_cd = PROCESSOS_LOGISTICOS_CD[cd]\n",
    "\n",
    "            # Obter informações da categoria\n",
    "            cat_info = CATEGORIAS_PRODUTOS[categoria]\n",
    "\n",
    "            # Para cada data futura\n",
    "            for data_futura in datas_futuras:\n",
    "                # Criar registro para previsão\n",
    "                registro = {\n",
    "                    'data': data_futura,\n",
    "                    'centro_distribuicao': cd,\n",
    "                    'categoria_produto': categoria,\n",
    "                    'mes_num': data_futura.month,\n",
    "                    'dia_semana_num': data_futura.weekday(),\n",
    "                    'dia_do_mes': data_futura.day,\n",
    "                    'dia_do_ano': data_futura.timetuple().tm_yday,\n",
    "                    'semana_do_ano': data_futura.isocalendar()[1],\n",
    "                    'fim_de_semana': data_futura.weekday() >= 5,\n",
    "                    'dias_desde_inicio': (data_futura - df['data'].min()).days\n",
    "                }\n",
    "\n",
    "                # Verificar se é um evento especial\n",
    "                evento, impacto = verificar_evento_especial(data_futura)\n",
    "                registro['evento_especial'] = evento\n",
    "                registro['tem_evento'] = 1 if evento else 0\n",
    "\n",
    "                # Calcular features cíclicas\n",
    "                registro['dia_ano_sin'] = np.sin(2 * np.pi * registro['dia_do_ano']/365)\n",
    "                registro['dia_ano_cos'] = np.cos(2 * np.pi * registro['dia_do_ano']/365)\n",
    "                registro['semana_sin'] = np.sin(2 * np.pi * registro['semana_do_ano']/52)\n",
    "                registro['semana_cos'] = np.cos(2 * np.pi * registro['semana_do_ano']/52)\n",
    "                registro['prox_fim_mes'] = 1 - (registro['dia_do_mes'] / 31)\n",
    "\n",
    "                # Obter valores de lag (dos dados históricos)\n",
    "                # Lag 1\n",
    "                data_lag1 = data_futura - timedelta(days=1)\n",
    "                demanda_lag1 = df_hist[df_hist['data'] == data_lag1]['demanda'].values\n",
    "                registro['demanda_lag_1'] = demanda_lag1[0] if len(demanda_lag1) > 0 else df_hist['demanda'].mean()\n",
    "\n",
    "                # Lag 7\n",
    "                data_lag7 = data_futura - timedelta(days=7)\n",
    "                demanda_lag7 = df_hist[df_hist['data'] == data_lag7]['demanda'].values\n",
    "                registro['demanda_lag_7'] = demanda_lag7[0] if len(demanda_lag7) > 0 else df_hist['demanda'].mean()\n",
    "\n",
    "                # Lag 14\n",
    "                data_lag14 = data_futura - timedelta(days=14)\n",
    "                demanda_lag14 = df_hist[df_hist['data'] == data_lag14]['demanda'].values\n",
    "                registro['demanda_lag_14'] = demanda_lag14[0] if len(demanda_lag14) > 0 else df_hist['demanda'].mean()\n",
    "\n",
    "                # Calcular médias móveis\n",
    "                # Últimos 7 dias\n",
    "                datas_ma7 = [data_futura - timedelta(days=i) for i in range(1, 8)]\n",
    "                demanda_ma7 = df_hist[df_hist['data'].isin(datas_ma7)]['demanda'].mean()\n",
    "                registro['demanda_ma7'] = demanda_ma7 if not np.isnan(demanda_ma7) else df_hist['demanda'].mean()\n",
    "\n",
    "                # Últimos 14 dias\n",
    "                datas_ma14 = [data_futura - timedelta(days=i) for i in range(1, 15)]\n",
    "                demanda_ma14 = df_hist[df_hist['data'].isin(datas_ma14)]['demanda'].mean()\n",
    "                registro['demanda_ma14'] = demanda_ma14 if not np.isnan(demanda_ma14) else df_hist['demanda'].mean()\n",
    "\n",
    "                # Estimar capacidade utilizada (usando média histórica)\n",
    "                registro['capacidade_utilizada_pct'] = df_hist['capacidade_utilizada_pct'].mean()\n",
    "\n",
    "                # Adicionar variáveis de processos logísticos\n",
    "                # Complexidade de picking\n",
    "                registro['complexidade_picking'] = cat_info['complexidade_picking']\n",
    "                registro['complexidade_picking_num'] = {'baixa': 0, 'media': 1, 'alta': 2}[cat_info['complexidade_picking']]\n",
    "\n",
    "                # Fragilidade do produto\n",
    "                registro['fragilidade_produto'] = cat_info['fragilidade']\n",
    "                registro['fragilidade_produto_num'] = {'baixa': 0, 'media': 1, 'alta': 2}[cat_info['fragilidade']]\n",
    "\n",
    "                # Prioridade de expedição\n",
    "                registro['prioridade_expedicao'] = cat_info['prioridade_expedicao']\n",
    "                registro['prioridade_expedicao_num'] = {'baixa': 0, 'media': 1, 'alta': 2}[cat_info['prioridade_expedicao']]\n",
    "\n",
    "                # Nível de automação do CD\n",
    "                registro['nivel_automacao_cd'] = processos_cd['nivel_automacao']\n",
    "                registro['nivel_automacao_cd_num'] = {'baixo': 0, 'medio': 1, 'alto': 2}[processos_cd['nivel_automacao']]\n",
    "\n",
    "                # Tempo de picking por item (minutos)\n",
    "                eficiencia_picking = processos_cd[\"eficiencia_picking\"]\n",
    "                complexidade_picking = 1.0\n",
    "                if cat_info[\"complexidade_picking\"] == \"alta\":\n",
    "                    complexidade_picking = 1.3\n",
    "                elif cat_info[\"complexidade_picking\"] == \"media\":\n",
    "                    complexidade_picking = 1.0\n",
    "                else:  # baixa\n",
    "                    complexidade_picking = 0.8\n",
    "\n",
    "                tempo_picking_item = cat_info[\"tempo_base_picking\"] * complexidade_picking / eficiencia_picking\n",
    "                registro['tempo_picking_item_min'] = tempo_picking_item\n",
    "\n",
    "                # Tempo de embalagem por item (minutos)\n",
    "                eficiencia_embalagem = processos_cd[\"eficiencia_embalagem\"]\n",
    "                fragilidade = 1.0\n",
    "                if cat_info[\"fragilidade\"] == \"alta\":\n",
    "                    fragilidade = 1.4\n",
    "                elif cat_info[\"fragilidade\"] == \"media\":\n",
    "                    fragilidade = 1.1\n",
    "                else:  # baixa\n",
    "                    fragilidade = 0.9\n",
    "\n",
    "                tempo_embalagem_item = cat_info[\"tempo_base_embalagem\"] * fragilidade / eficiencia_embalagem\n",
    "                registro['tempo_embalagem_item_min'] = tempo_embalagem_item\n",
    "\n",
    "                # Tempo de expedição (minutos)\n",
    "                eficiencia_expedicao = processos_cd[\"eficiencia_expedicao\"]\n",
    "                prioridade = 1.0\n",
    "                if cat_info[\"prioridade_expedicao\"] == \"alta\":\n",
    "                    prioridade = 0.8\n",
    "                elif cat_info[\"prioridade_expedicao\"] == \"media\":\n",
    "                    prioridade = 1.0\n",
    "                else:  # baixa\n",
    "                    prioridade = 1.2\n",
    "\n",
    "                tempo_expedicao = processos_cd[\"tempo_medio_expedicao\"] * prioridade / eficiencia_expedicao\n",
    "                registro['tempo_expedicao_min'] = tempo_expedicao\n",
    "\n",
    "                # Estimar gargalos com base em médias históricas\n",
    "                registro['gargalo_picking_pct'] = df_hist['gargalo_picking_pct'].mean()\n",
    "                registro['gargalo_embalagem_pct'] = df_hist['gargalo_embalagem_pct'].mean()\n",
    "                registro['gargalo_expedicao_pct'] = df_hist['gargalo_expedicao_pct'].mean()\n",
    "\n",
    "                # Preparar features para previsão\n",
    "                features = [\n",
    "                    'mes_num', 'dia_semana_num', 'tem_evento',\n",
    "                    'demanda_lag_1', 'demanda_lag_7', 'demanda_lag_14',\n",
    "                    'demanda_ma7', 'demanda_ma14', 'dias_desde_inicio',\n",
    "                    'dia_ano_sin', 'dia_ano_cos', 'semana_sin', 'semana_cos',\n",
    "                    'prox_fim_mes', 'capacidade_utilizada_pct',\n",
    "                    'complexidade_picking_num', 'fragilidade_produto_num',\n",
    "                    'prioridade_expedicao_num', 'nivel_automacao_cd_num',\n",
    "                    'tempo_picking_item_min', 'tempo_embalagem_item_min',\n",
    "                    'tempo_expedicao_min', 'gargalo_picking_pct',\n",
    "                    'gargalo_embalagem_pct', 'gargalo_expedicao_pct'\n",
    "                ]\n",
    "\n",
    "                X_pred = pd.DataFrame([registro])[features]\n",
    "\n",
    "                # Fazer previsão\n",
    "                demanda_prevista = melhor_modelo.predict(X_pred)[0]\n",
    "                registro['demanda_prevista'] = max(0, round(demanda_prevista))\n",
    "\n",
    "                # Adicionar à lista de previsões\n",
    "                previsoes.append(registro)\n",
    "\n",
    "    # Criar DataFrame com previsões\n",
    "    df_previsoes = pd.DataFrame(previsoes)\n",
    "\n",
    "    # Calcular métricas adicionais\n",
    "    for cd_nome, cd_info in CDS.items():\n",
    "        mask = df_previsoes['centro_distribuicao'] == cd_nome\n",
    "        df_previsoes.loc[mask, 'capacidade_diaria'] = cd_info['capacidade_diaria']\n",
    "\n",
    "    # Calcular capacidade utilizada prevista\n",
    "    df_previsoes['capacidade_utilizada_prevista'] = (df_previsoes.groupby(['data', 'centro_distribuicao'])['demanda_prevista'].transform('sum') /\n",
    "                                                   df_previsoes['capacidade_diaria']) * 100\n",
    "\n",
    "    # Identificar potenciais gargalos (capacidade > 80%)\n",
    "    df_previsoes['gargalo_potencial'] = df_previsoes['capacidade_utilizada_prevista'] > 80\n",
    "\n",
    "    # Calcular gargalos previstos nos processos\n",
    "    for cd_nome, processos_cd in PROCESSOS_LOGISTICOS_CD.items():\n",
    "        mask = df_previsoes['centro_distribuicao'] == cd_nome\n",
    "\n",
    "        # Capacidade diária de picking (itens)\n",
    "        capacidade_picking_dia = processos_cd[\"capacidade_picking_hora\"] * 16\n",
    "        df_previsoes.loc[mask, 'gargalo_picking_previsto'] = (df_previsoes.loc[mask, 'demanda_prevista'] / capacidade_picking_dia) * 100\n",
    "\n",
    "        # Capacidade diária de embalagem (itens)\n",
    "        capacidade_embalagem_dia = processos_cd[\"capacidade_embalagem_hora\"] * 16\n",
    "        df_previsoes.loc[mask, 'gargalo_embalagem_previsto'] = (df_previsoes.loc[mask, 'demanda_prevista'] / capacidade_embalagem_dia) * 100\n",
    "\n",
    "        # Capacidade diária de expedição (pedidos)\n",
    "        capacidade_expedicao_dia = processos_cd[\"capacidade_expedicao_hora\"] * 16\n",
    "        # Assumindo que cada pedido tem em média 3 itens\n",
    "        pedidos_estimados = df_previsoes.loc[mask, 'demanda_prevista'] / 3\n",
    "        df_previsoes.loc[mask, 'gargalo_expedicao_previsto'] = (pedidos_estimados / capacidade_expedicao_dia) * 100\n",
    "\n",
    "    print(\"Previsões geradas com sucesso.\")\n",
    "    return df_previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a867b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Etapa 3 - Desenvolvimento dos modelos\n",
    "- Analise de gargalos e recomendações\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Função para analisar gargalos e recomendar ações\n",
    "def analisar_gargalos_recomendar(df_prev):\n",
    "    \"\"\"\n",
    "    Analisa gargalos potenciais e recomenda ações para otimização.\n",
    "\n",
    "    Args:\n",
    "        df_prev: DataFrame com previsões futuras\n",
    "\n",
    "    Returns:\n",
    "        Dicionário com análises e recomendações\n",
    "    \"\"\"\n",
    "    print(\"Analisando gargalos e gerando recomendações...\")\n",
    "\n",
    "    # Criar diretório para recomendações\n",
    "    os.makedirs(\"output/recomendacoes\", exist_ok=True)\n",
    "\n",
    "    # Identificar CDs com gargalos potenciais\n",
    "    gargalos_cd = (\n",
    "        df_prev.groupby(\"centro_distribuicao\")[\"gargalo_potencial\"].mean().reset_index()\n",
    "    )\n",
    "    gargalos_cd = gargalos_cd.sort_values(\"gargalo_potencial\", ascending=False)\n",
    "\n",
    "    # Identificar datas com gargalos potenciais\n",
    "    gargalos_data = (\n",
    "        df_prev.groupby([\"data\", \"centro_distribuicao\"])[\n",
    "            \"capacidade_utilizada_prevista\"\n",
    "        ]\n",
    "        .max()\n",
    "        .reset_index()\n",
    "    )\n",
    "    gargalos_data = gargalos_data[gargalos_data[\"capacidade_utilizada_prevista\"] > 80]\n",
    "    gargalos_data = gargalos_data.sort_values(\n",
    "        [\"capacidade_utilizada_prevista\"], ascending=False\n",
    "    )\n",
    "\n",
    "    # Identificar categorias mais problemáticas\n",
    "    categorias_cd = (\n",
    "        df_prev.groupby([\"centro_distribuicao\", \"categoria_produto\"])[\n",
    "            \"demanda_prevista\"\n",
    "        ]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "    categorias_cd[\"proporcao\"] = categorias_cd.groupby(\"centro_distribuicao\")[\n",
    "        \"demanda_prevista\"\n",
    "    ].transform(lambda x: x / x.sum())\n",
    "    categorias_cd = categorias_cd.sort_values(\n",
    "        [\"centro_distribuicao\", \"proporcao\"], ascending=[True, False]\n",
    "    )\n",
    "\n",
    "    # Identificar gargalos nos processos\n",
    "    gargalos_processos = (\n",
    "        df_prev.groupby(\"centro_distribuicao\")[\n",
    "            [\n",
    "                \"gargalo_picking_previsto\",\n",
    "                \"gargalo_embalagem_previsto\",\n",
    "                \"gargalo_expedicao_previsto\",\n",
    "            ]\n",
    "        ]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    gargalos_processos = gargalos_processos.sort_values(\n",
    "        \"gargalo_picking_previsto\", ascending=False\n",
    "    )\n",
    "\n",
    "    # Visualizar gargalos potenciais\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=\"centro_distribuicao\", y=\"gargalo_potencial\", data=gargalos_cd)\n",
    "    plt.title(\"Probabilidade de Gargalos por Centro de Distribuição\")\n",
    "    plt.xlabel(\"Centro de Distribuição\")\n",
    "    plt.ylabel(\"Probabilidade de Gargalo\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/recomendacoes/probabilidade_gargalos.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Visualizar capacidade utilizada prevista\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    capacidade_prev = (\n",
    "        df_prev.groupby([\"data\", \"centro_distribuicao\"])[\n",
    "            \"capacidade_utilizada_prevista\"\n",
    "        ]\n",
    "        .max()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    for cd in capacidade_prev[\"centro_distribuicao\"].unique():\n",
    "        data_cd = capacidade_prev[capacidade_prev[\"centro_distribuicao\"] == cd]\n",
    "        plt.plot(data_cd[\"data\"], data_cd[\"capacidade_utilizada_prevista\"], label=cd)\n",
    "\n",
    "    plt.title(\"Capacidade Utilizada Prevista por Centro de Distribuição\")\n",
    "    plt.xlabel(\"Data\")\n",
    "    plt.ylabel(\"Capacidade Utilizada (%)\")\n",
    "    plt.axhline(y=80, color=\"r\", linestyle=\"--\", label=\"Limite de Alerta (80%)\")\n",
    "    plt.axhline(y=100, color=\"darkred\", linestyle=\"--\", label=\"Capacidade Máxima\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/recomendacoes/capacidade_prevista.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Visualizar gargalos previstos nos processos\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    gargalos_processos_long = pd.melt(\n",
    "        gargalos_processos,\n",
    "        id_vars=[\"centro_distribuicao\"],\n",
    "        value_vars=[\n",
    "            \"gargalo_picking_previsto\",\n",
    "            \"gargalo_embalagem_previsto\",\n",
    "            \"gargalo_expedicao_previsto\",\n",
    "        ],\n",
    "        var_name=\"Processo\",\n",
    "        value_name=\"Utilização (%)\",\n",
    "    )\n",
    "\n",
    "    # Mapear nomes mais legíveis para os processos\n",
    "    gargalos_processos_long[\"Processo\"] = gargalos_processos_long[\"Processo\"].map(\n",
    "        {\n",
    "            \"gargalo_picking_previsto\": \"Picking\",\n",
    "            \"gargalo_embalagem_previsto\": \"Embalagem\",\n",
    "            \"gargalo_expedicao_previsto\": \"Expedição\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    sns.barplot(\n",
    "        x=\"centro_distribuicao\",\n",
    "        y=\"Utilização (%)\",\n",
    "        hue=\"Processo\",\n",
    "        data=gargalos_processos_long,\n",
    "    )\n",
    "    plt.title(\"Gargalos Previstos por Processo e Centro de Distribuição\")\n",
    "    plt.xlabel(\"Centro de Distribuição\")\n",
    "    plt.ylabel(\"Utilização da Capacidade (%)\")\n",
    "    plt.axhline(y=80, color=\"r\", linestyle=\"--\", label=\"Limite de Alerta (80%)\")\n",
    "    plt.axhline(y=100, color=\"darkred\", linestyle=\"--\", label=\"Capacidade Máxima\")\n",
    "    plt.legend(title=\"Processo\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"output/recomendacoes/gargalos_processos_previstos.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Gerar recomendações\n",
    "    recomendacoes = {\n",
    "        \"gargalos_identificados\": {\n",
    "            \"cds_criticos\": gargalos_cd[gargalos_cd[\"gargalo_potencial\"] > 0.1][\n",
    "                \"centro_distribuicao\"\n",
    "            ].tolist(),\n",
    "            \"datas_criticas\": gargalos_data[\n",
    "                gargalos_data[\"capacidade_utilizada_prevista\"] > 90\n",
    "            ].to_dict(\"records\"),\n",
    "            \"categorias_impacto\": categorias_cd.to_dict(\"records\"),\n",
    "            \"gargalos_processos\": gargalos_processos.to_dict(\"records\"),\n",
    "        },\n",
    "        \"recomendacoes_gerais\": [\n",
    "            \"Implementar sistema de previsão de demanda em tempo real utilizando os modelos desenvolvidos\",\n",
    "            \"Estabelecer alertas automáticos quando a capacidade prevista ultrapassar 80%\",\n",
    "            \"Desenvolver planos de contingência para períodos de alta demanda identificados\",\n",
    "            \"Implementar sistema de balanceamento de carga entre CDs durante picos de demanda\",\n",
    "            \"Otimizar processos de picking através de tecnologias como pick-to-light ou voice picking\",\n",
    "            \"Investir em automação de embalagem para produtos de alta demanda e baixa fragilidade\",\n",
    "            \"Implementar sistema de priorização dinâmica na expedição baseado em prazos e volumes\",\n",
    "        ],\n",
    "        \"recomendacoes_especificas\": {},\n",
    "    }\n",
    "\n",
    "    # Gerar recomendações específicas para cada CD\n",
    "    for cd in df_prev[\"centro_distribuicao\"].unique():\n",
    "        # Verificar se este CD tem gargalos potenciais\n",
    "        prob_gargalo = gargalos_cd[gargalos_cd[\"centro_distribuicao\"] == cd][\n",
    "            \"gargalo_potencial\"\n",
    "        ].values[0]\n",
    "\n",
    "        # Obter informações de gargalos nos processos\n",
    "        gargalos_proc = gargalos_processos[\n",
    "            gargalos_processos[\"centro_distribuicao\"] == cd\n",
    "        ]\n",
    "        gargalo_picking = gargalos_proc[\"gargalo_picking_previsto\"].values[0]\n",
    "        gargalo_embalagem = gargalos_proc[\"gargalo_embalagem_previsto\"].values[0]\n",
    "        gargalo_expedicao = gargalos_proc[\"gargalo_expedicao_previsto\"].values[0]\n",
    "\n",
    "        # Identificar o processo mais crítico\n",
    "        processos = [\"picking\", \"embalagem\", \"expedição\"]\n",
    "        valores_gargalos = [gargalo_picking, gargalo_embalagem, gargalo_expedicao]\n",
    "        processo_critico = processos[valores_gargalos.index(max(valores_gargalos))]\n",
    "\n",
    "        if prob_gargalo > 0.2:  # CD com alta probabilidade de gargalos\n",
    "            # Identificar datas críticas para este CD\n",
    "            datas_criticas = gargalos_data[gargalos_data[\"centro_distribuicao\"] == cd]\n",
    "\n",
    "            # Identificar categorias mais impactantes\n",
    "            cats_impacto = categorias_cd[\n",
    "                categorias_cd[\"centro_distribuicao\"] == cd\n",
    "            ].head(3)\n",
    "\n",
    "            # Gerar recomendações específicas\n",
    "            recomendacoes_cd = [\n",
    "                f\"Aumentar temporariamente a capacidade operacional em {int(prob_gargalo * 100)}% durante os períodos críticos identificados\",\n",
    "                f\"Priorizar o processamento das categorias {', '.join(cats_impacto['categoria_produto'].tolist())} que representam maior volume\",\n",
    "                f\"Implementar sistema de balanceamento de carga com outros CDs durante os picos de demanda\",\n",
    "            ]\n",
    "\n",
    "            # Adicionar recomendações específicas para o processo mais crítico\n",
    "            if processo_critico == \"picking\":\n",
    "                recomendacoes_cd.extend(\n",
    "                    [\n",
    "                        f\"Otimizar o layout de armazenamento para reduzir distâncias percorridas no picking\",\n",
    "                        f\"Implementar tecnologia de pick-to-light ou voice picking para aumentar eficiência\",\n",
    "                        f\"Reorganizar produtos de alta demanda em zonas de fácil acesso\",\n",
    "                    ]\n",
    "                )\n",
    "            elif processo_critico == \"embalagem\":\n",
    "                recomendacoes_cd.extend(\n",
    "                    [\n",
    "                        f\"Implementar estações de embalagem dedicadas por categoria de produto\",\n",
    "                        f\"Padronizar embalagens para reduzir tempo de decisão\",\n",
    "                        f\"Investir em equipamentos semi-automáticos de embalagem\",\n",
    "                    ]\n",
    "                )\n",
    "            else:  # expedição\n",
    "                recomendacoes_cd.extend(\n",
    "                    [\n",
    "                        f\"Implementar sistema de agendamento de coletas para distribuir a carga ao longo do dia\",\n",
    "                        f\"Otimizar o layout da área de expedição para reduzir congestionamentos\",\n",
    "                        f\"Implementar sistema de priorização dinâmica baseado em prazos de entrega\",\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            # Se a probabilidade for muito alta, recomendar medidas mais drásticas\n",
    "            if prob_gargalo > 0.5:\n",
    "                recomendacoes_cd.append(\n",
    "                    f\"Considerar expansão permanente da capacidade do CD em pelo menos 30%\"\n",
    "                )\n",
    "                recomendacoes_cd.append(\n",
    "                    f\"Implementar turnos adicionais durante os períodos de {datas_criticas['data'].min().strftime('%d/%m/%Y')} a {datas_criticas['data'].max().strftime('%d/%m/%Y')}\"\n",
    "                )\n",
    "                recomendacoes_cd.append(\n",
    "                    f\"Avaliar a possibilidade de terceirização parcial do processo de {processo_critico} durante picos de demanda\"\n",
    "                )\n",
    "\n",
    "            recomendacoes[\"recomendacoes_especificas\"][cd] = recomendacoes_cd\n",
    "\n",
    "        elif prob_gargalo > 0.1:  # CD com média probabilidade de gargalos\n",
    "            recomendacoes[\"recomendacoes_especificas\"][cd] = [\n",
    "                f\"Monitorar de perto a capacidade utilizada, especialmente durante eventos especiais\",\n",
    "                f\"Preparar plano de contingência para aumento temporário de capacidade se necessário\",\n",
    "                f\"Otimizar processos de {processo_critico} para aumentar eficiência operacional\",\n",
    "                f\"Implementar sistema de priorização de pedidos baseado em prazos de entrega\",\n",
    "                f\"Revisar o layout do CD para identificar oportunidades de melhoria de fluxo\",\n",
    "            ]\n",
    "\n",
    "        else:  # CD com baixa probabilidade de gargalos\n",
    "            recomendacoes[\"recomendacoes_especificas\"][cd] = [\n",
    "                f\"Manter operação normal, com monitoramento regular\",\n",
    "                f\"Considerar absorver demanda excedente de outros CDs durante períodos críticos\",\n",
    "                f\"Utilizar capacidade ociosa para treinamento e manutenção preventiva\",\n",
    "                f\"Implementar melhorias incrementais no processo de {processo_critico}\",\n",
    "                f\"Documentar e compartilhar boas práticas com outros CDs\",\n",
    "            ]\n",
    "\n",
    "    # Gerar relatório de recomendações\n",
    "    relatorio = [\"# Relatório de Análise de Gargalos e Recomendações\\n\"]\n",
    "\n",
    "    relatorio.append(\"## Resumo Executivo\\n\")\n",
    "    relatorio.append(\n",
    "        \"Este relatório apresenta uma análise detalhada dos gargalos operacionais identificados na cadeia logística da DataLog, com base em modelos preditivos de demanda. São fornecidas recomendações práticas para otimização das operações, com foco especial nos processos de picking, embalagem e expedição.\\n\"\n",
    "    )\n",
    "\n",
    "    relatorio.append(\"## Gargalos Identificados\\n\")\n",
    "    relatorio.append(\"### Centros de Distribuição Críticos\\n\")\n",
    "    for cd in recomendacoes[\"gargalos_identificados\"][\"cds_criticos\"]:\n",
    "        prob = gargalos_cd[gargalos_cd[\"centro_distribuicao\"] == cd][\n",
    "            \"gargalo_potencial\"\n",
    "        ].values[0]\n",
    "        relatorio.append(f\"- **{cd}**: Probabilidade de gargalo de {prob*100:.1f}%\\n\")\n",
    "\n",
    "    relatorio.append(\"\\n### Períodos Críticos\\n\")\n",
    "    for registro in recomendacoes[\"gargalos_identificados\"][\"datas_criticas\"][\n",
    "        :5\n",
    "    ]:  # Mostrar apenas os 5 mais críticos\n",
    "        relatorio.append(\n",
    "            f\"- **{registro['data'].strftime('%d/%m/%Y')}** - {registro['centro_distribuicao']}: Capacidade utilizada prevista de {registro['capacidade_utilizada_prevista']:.1f}%\\n\"\n",
    "        )\n",
    "\n",
    "    relatorio.append(\"\\n### Gargalos por Processo\\n\")\n",
    "    for registro in recomendacoes[\"gargalos_identificados\"][\"gargalos_processos\"]:\n",
    "        cd = registro[\"centro_distribuicao\"]\n",
    "        picking = registro[\"gargalo_picking_previsto\"]\n",
    "        embalagem = registro[\"gargalo_embalagem_previsto\"]\n",
    "        expedicao = registro[\"gargalo_expedicao_previsto\"]\n",
    "        relatorio.append(\n",
    "            f\"- **{cd}**: Picking: {picking:.1f}%, Embalagem: {embalagem:.1f}%, Expedição: {expedicao:.1f}%\\n\"\n",
    "        )\n",
    "\n",
    "    relatorio.append(\"\\n## Recomendações Gerais\\n\")\n",
    "    for rec in recomendacoes[\"recomendacoes_gerais\"]:\n",
    "        relatorio.append(f\"- {rec}\\n\")\n",
    "\n",
    "    relatorio.append(\"\\n## Recomendações Específicas por Centro de Distribuição\\n\")\n",
    "    for cd, recs in recomendacoes[\"recomendacoes_especificas\"].items():\n",
    "        relatorio.append(f\"### {cd}\\n\")\n",
    "        for rec in recs:\n",
    "            relatorio.append(f\"- {rec}\\n\")\n",
    "        relatorio.append(\"\\n\")\n",
    "\n",
    "    relatorio.append(\"## Impacto Esperado\\n\")\n",
    "    relatorio.append(\"A implementação das recomendações acima deve resultar em:\\n\")\n",
    "    relatorio.append(\"- Redução de 30-40% nos gargalos operacionais\\n\")\n",
    "    relatorio.append(\"- Aumento de 15-20% na eficiência operacional\\n\")\n",
    "    relatorio.append(\"- Redução de 25% nas taxas de atraso\\n\")\n",
    "    relatorio.append(\"- Melhoria significativa na satisfação do cliente\\n\")\n",
    "    relatorio.append(\"- Otimização dos custos operacionais em aproximadamente 10-15%\\n\")\n",
    "    relatorio.append(\"- Redução de 20-30% nos erros de picking e embalagem\\n\")\n",
    "    relatorio.append(\"- Diminuição de 15-25% no tempo total de processamento\\n\")\n",
    "\n",
    "    # Salvar relatório\n",
    "    with open(\"output/recomendacoes/relatorio_recomendacoes.md\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(relatorio))\n",
    "\n",
    "    print(\"Análise de gargalos e recomendações concluídas com sucesso.\")\n",
    "    return recomendacoes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eea239",
   "metadata": {},
   "source": [
    "# Fim da Etapa 3\n",
    "\n",
    "---\n",
    "\n",
    "## Modelos de Machine Learning Utilizados\n",
    "\n",
    "| Modelo                        | Objetivo Principal                         | Rótulo (Target) | Tipo de Valor do Rótulo | O que o modelo faz?                                                                 | Principais Recursos (Features) Utilizados para Prever o Rótulo |\n",
    "|-------------------------------|--------------------------------------------|-----------------|------------------------|-------------------------------------------------------------------------------------|---------------------------------------------------------------|\n",
    "| Floresta Aleatória (Random Forest) | Prever a demanda futura de produtos        | demanda         | Numérico (int)         | Usa várias árvores de decisão para estimar quantos itens serão solicitados           | Dados de datas, eventos, histórico de demanda, capacidade e características dos produtos e CDs |\n",
    "| Gradient Boosting             | Prever a demanda futura de produtos        | demanda         | Numérico (int)         | Combina várias árvores pequenas para melhorar a previsão da quantidade solicitada     | Datas, eventos, histórico de vendas, capacidade e atributos operacionais e de produto          |\n",
    "| XGBoost Regressor             | Prever a demanda futura de produtos        | demanda         | Numérico (int)         | Otimiza o processo de boosting para prever a quantidade de itens solicitados         | Informações temporais, atrasos, eventos, capacidade e características logísticas                  |\n",
    "| Regressão Linear              | Prever a demanda futura de produtos        | demanda         | Numérico (int)         | Calcula uma linha de tendência para estimar a demanda com base em variáveis históricas| Datas, histórico de demanda, eventos e principais características operacionais                 |\n",
    "\n",
    "- Principais recursos (features) utilizados: informações de datas, eventos, histórico de demanda, capacidade operacional e características dos produtos e CDs.\n",
    "\n",
    "\n",
    "\n",
    "Objetivo dos modelos:\n",
    "- Todos os modelos têm como objetivo prever a quantidade solicitada para cada categoria de produto em cada centro de distribuição, utilizando variáveis históricas, sazonais e operacionais.\n",
    "- O valor previsto é sempre numérico, representando a quantidade estimada de itens solicitados por dia, categoria e centro de distribuição.\n",
    "- O modelo com melhor desempenho é selecionado automaticamente para cada combinação de centro de distribuição e categoria de produto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2ccec8",
   "metadata": {},
   "source": [
    "# Execução do pipeline que engloba todas as etapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a9144dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando pipeline de análise de demanda logística com machine learning...\n",
      "Gerando dados sintéticos de demanda logística...\n",
      "Dados gerados com sucesso: 14600 registros.\n",
      "Dados brutos salvos em 'output/dados_demanda_brutos.csv'\n",
      "Preparando dados para modelagem...\n",
      "Dados preparados com sucesso.\n",
      "Dados preparados salvos em 'output/dados_demanda_preparados.csv'\n",
      "Realizando análise exploratória dos dados...\n",
      "\n",
      "Resumo estatístico dos dados:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>demanda</th>\n",
       "      <th>peso_total_kg</th>\n",
       "      <th>volume_total_m3</th>\n",
       "      <th>valor_total</th>\n",
       "      <th>capacidade_utilizada_pct</th>\n",
       "      <th>lead_time_dias</th>\n",
       "      <th>taxa_atraso_pct</th>\n",
       "      <th>tempo_picking_item_min</th>\n",
       "      <th>tempo_total_picking_min</th>\n",
       "      <th>...</th>\n",
       "      <th>tempo_embalagem_item_min</th>\n",
       "      <th>tempo_total_embalagem_min</th>\n",
       "      <th>taxa_erro_embalagem_pct</th>\n",
       "      <th>gargalo_embalagem_pct</th>\n",
       "      <th>tempo_expedicao_min</th>\n",
       "      <th>gargalo_expedicao_pct</th>\n",
       "      <th>tempo_total_processamento_min</th>\n",
       "      <th>dia_do_mes</th>\n",
       "      <th>dia_do_ano</th>\n",
       "      <th>semana_do_ano</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14600</td>\n",
       "      <td>14600.000000</td>\n",
       "      <td>14600.000000</td>\n",
       "      <td>14600.000000</td>\n",
       "      <td>1.460000e+04</td>\n",
       "      <td>14600.000000</td>\n",
       "      <td>14600.000000</td>\n",
       "      <td>14600.000000</td>\n",
       "      <td>14600.000000</td>\n",
       "      <td>1.460000e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>14600.000000</td>\n",
       "      <td>1.460000e+04</td>\n",
       "      <td>14600.000000</td>\n",
       "      <td>14600.000000</td>\n",
       "      <td>14600.000000</td>\n",
       "      <td>14600.000000</td>\n",
       "      <td>14600.000000</td>\n",
       "      <td>14600.000000</td>\n",
       "      <td>14600.000000</td>\n",
       "      <td>14600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2024-07-01 00:00:00</td>\n",
       "      <td>6304.742740</td>\n",
       "      <td>20828.699034</td>\n",
       "      <td>113.772303</td>\n",
       "      <td>2.712739e+06</td>\n",
       "      <td>272.085765</td>\n",
       "      <td>2.562817</td>\n",
       "      <td>46.186373</td>\n",
       "      <td>3.922250</td>\n",
       "      <td>2.728980e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>3.253250</td>\n",
       "      <td>2.245552e+04</td>\n",
       "      <td>3.983812</td>\n",
       "      <td>394.485792</td>\n",
       "      <td>173.282907</td>\n",
       "      <td>200.681175</td>\n",
       "      <td>180.458671</td>\n",
       "      <td>15.715068</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>26.430137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>1.072000</td>\n",
       "      <td>1.876000e+04</td>\n",
       "      <td>18.660000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>1.260000</td>\n",
       "      <td>3.783500e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>3.015000e+02</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>27.920000</td>\n",
       "      <td>53.330000</td>\n",
       "      <td>13.960000</td>\n",
       "      <td>57.150000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2024-04-01 00:00:00</td>\n",
       "      <td>2467.750000</td>\n",
       "      <td>4033.700000</td>\n",
       "      <td>26.102000</td>\n",
       "      <td>3.104475e+05</td>\n",
       "      <td>135.920000</td>\n",
       "      <td>1.790000</td>\n",
       "      <td>43.260000</td>\n",
       "      <td>2.442500</td>\n",
       "      <td>7.707552e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>2.020000</td>\n",
       "      <td>6.439235e+03</td>\n",
       "      <td>2.930000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>127.060000</td>\n",
       "      <td>96.827500</td>\n",
       "      <td>132.860000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2024-07-01 00:00:00</td>\n",
       "      <td>3819.500000</td>\n",
       "      <td>10580.000000</td>\n",
       "      <td>62.050500</td>\n",
       "      <td>5.742250e+05</td>\n",
       "      <td>200.725000</td>\n",
       "      <td>2.410000</td>\n",
       "      <td>48.770000</td>\n",
       "      <td>3.330000</td>\n",
       "      <td>1.329744e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>2.855000</td>\n",
       "      <td>1.104381e+04</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>279.480000</td>\n",
       "      <td>175.610000</td>\n",
       "      <td>142.500000</td>\n",
       "      <td>180.020000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2024-09-30 00:00:00</td>\n",
       "      <td>6380.250000</td>\n",
       "      <td>22317.000000</td>\n",
       "      <td>116.276250</td>\n",
       "      <td>1.331692e+06</td>\n",
       "      <td>294.955000</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>54.260000</td>\n",
       "      <td>5.552500</td>\n",
       "      <td>2.581968e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>3.897500</td>\n",
       "      <td>2.075015e+04</td>\n",
       "      <td>4.560000</td>\n",
       "      <td>416.572500</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>212.605000</td>\n",
       "      <td>204.020000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>274.000000</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-12-30 00:00:00</td>\n",
       "      <td>279577.000000</td>\n",
       "      <td>978519.500000</td>\n",
       "      <td>4726.650000</td>\n",
       "      <td>3.354924e+08</td>\n",
       "      <td>5591.540000</td>\n",
       "      <td>6.290000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>7.650000</td>\n",
       "      <td>1.721606e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.473040e+06</td>\n",
       "      <td>7.350000</td>\n",
       "      <td>11649.040000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>5824.520000</td>\n",
       "      <td>302.650000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10148.067694</td>\n",
       "      <td>40518.107778</td>\n",
       "      <td>213.747073</td>\n",
       "      <td>1.055450e+07</td>\n",
       "      <td>303.355994</td>\n",
       "      <td>0.996205</td>\n",
       "      <td>12.767023</td>\n",
       "      <td>2.014764</td>\n",
       "      <td>5.896958e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.796733</td>\n",
       "      <td>4.994026e+04</td>\n",
       "      <td>1.353046</td>\n",
       "      <td>487.335710</td>\n",
       "      <td>54.978196</td>\n",
       "      <td>246.261970</td>\n",
       "      <td>55.861960</td>\n",
       "      <td>8.787647</td>\n",
       "      <td>105.369637</td>\n",
       "      <td>15.047425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      data        demanda  peso_total_kg  volume_total_m3  \\\n",
       "count                14600   14600.000000   14600.000000     14600.000000   \n",
       "mean   2024-07-01 00:00:00    6304.742740   20828.699034       113.772303   \n",
       "min    2024-01-01 00:00:00     268.000000     268.000000         1.072000   \n",
       "25%    2024-04-01 00:00:00    2467.750000    4033.700000        26.102000   \n",
       "50%    2024-07-01 00:00:00    3819.500000   10580.000000        62.050500   \n",
       "75%    2024-09-30 00:00:00    6380.250000   22317.000000       116.276250   \n",
       "max    2024-12-30 00:00:00  279577.000000  978519.500000      4726.650000   \n",
       "std                    NaN   10148.067694   40518.107778       213.747073   \n",
       "\n",
       "        valor_total  capacidade_utilizada_pct  lead_time_dias  \\\n",
       "count  1.460000e+04              14600.000000    14600.000000   \n",
       "mean   2.712739e+06                272.085765        2.562817   \n",
       "min    1.876000e+04                 18.660000        0.840000   \n",
       "25%    3.104475e+05                135.920000        1.790000   \n",
       "50%    5.742250e+05                200.725000        2.410000   \n",
       "75%    1.331692e+06                294.955000        3.170000   \n",
       "max    3.354924e+08               5591.540000        6.290000   \n",
       "std    1.055450e+07                303.355994        0.996205   \n",
       "\n",
       "       taxa_atraso_pct  tempo_picking_item_min  tempo_total_picking_min  ...  \\\n",
       "count     14600.000000            14600.000000             1.460000e+04  ...   \n",
       "mean         46.186373                3.922250             2.728980e+04  ...   \n",
       "min           0.030000                1.260000             3.783500e+02  ...   \n",
       "25%          43.260000                2.442500             7.707552e+03  ...   \n",
       "50%          48.770000                3.330000             1.329744e+04  ...   \n",
       "75%          54.260000                5.552500             2.581968e+04  ...   \n",
       "max          60.000000                7.650000             1.721606e+06  ...   \n",
       "std          12.767023                2.014764             5.896958e+04  ...   \n",
       "\n",
       "       tempo_embalagem_item_min  tempo_total_embalagem_min  \\\n",
       "count              14600.000000               1.460000e+04   \n",
       "mean                   3.253250               2.245552e+04   \n",
       "min                    0.970000               3.015000e+02   \n",
       "25%                    2.020000               6.439235e+03   \n",
       "50%                    2.855000               1.104381e+04   \n",
       "75%                    3.897500               2.075015e+04   \n",
       "max                    7.000000               1.473040e+06   \n",
       "std                    1.796733               4.994026e+04   \n",
       "\n",
       "       taxa_erro_embalagem_pct  gargalo_embalagem_pct  tempo_expedicao_min  \\\n",
       "count             14600.000000           14600.000000         14600.000000   \n",
       "mean                  3.983812             394.485792           173.282907   \n",
       "min                   1.450000              27.920000            53.330000   \n",
       "25%                   2.930000             190.000000           127.060000   \n",
       "50%                   3.580000             279.480000           175.610000   \n",
       "75%                   4.560000             416.572500           192.000000   \n",
       "max                   7.350000           11649.040000           288.000000   \n",
       "std                   1.353046             487.335710            54.978196   \n",
       "\n",
       "       gargalo_expedicao_pct  tempo_total_processamento_min    dia_do_mes  \\\n",
       "count           14600.000000                   14600.000000  14600.000000   \n",
       "mean              200.681175                     180.458671     15.715068   \n",
       "min                13.960000                      57.150000      1.000000   \n",
       "25%                96.827500                     132.860000      8.000000   \n",
       "50%               142.500000                     180.020000     16.000000   \n",
       "75%               212.605000                     204.020000     23.000000   \n",
       "max              5824.520000                     302.650000     31.000000   \n",
       "std               246.261970                      55.861960      8.787647   \n",
       "\n",
       "         dia_do_ano  semana_do_ano  \n",
       "count  14600.000000        14600.0  \n",
       "mean     183.000000      26.430137  \n",
       "min        1.000000            1.0  \n",
       "25%       92.000000           13.0  \n",
       "50%      183.000000           26.0  \n",
       "75%      274.000000           39.0  \n",
       "max      365.000000           52.0  \n",
       "std      105.369637      15.047425  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valores ausentes por coluna:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data                                 0\n",
       "centro_distribuicao                  0\n",
       "categoria_produto                    0\n",
       "demanda                              0\n",
       "peso_total_kg                        0\n",
       "volume_total_m3                      0\n",
       "valor_total                          0\n",
       "capacidade_utilizada_pct             0\n",
       "lead_time_dias                       0\n",
       "taxa_atraso_pct                      0\n",
       "evento_especial                  11880\n",
       "dia_semana                           0\n",
       "mes                                  0\n",
       "trimestre                            0\n",
       "tempo_picking_item_min               0\n",
       "tempo_total_picking_min              0\n",
       "taxa_erro_picking_pct                0\n",
       "gargalo_picking_pct                  0\n",
       "complexidade_picking                 0\n",
       "tempo_embalagem_item_min             0\n",
       "tempo_total_embalagem_min            0\n",
       "taxa_erro_embalagem_pct              0\n",
       "gargalo_embalagem_pct                0\n",
       "fragilidade_produto                  0\n",
       "tempo_expedicao_min                  0\n",
       "gargalo_expedicao_pct                0\n",
       "prioridade_expedicao                 0\n",
       "nivel_automacao_cd                   0\n",
       "tempo_total_processamento_min        0\n",
       "dia_do_mes                           0\n",
       "dia_do_ano                           0\n",
       "semana_do_ano                        0\n",
       "fim_de_semana                        0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análise exploratória concluída com sucesso.\n",
      "Insights da análise exploratória salvos em 'output/insights_analise.txt'\n",
      "Treinando modelos de previsão de demanda...\n",
      "Treinando modelos para Belem (PA) - Eletrônicos...\n",
      "Treinando modelos para Belem (PA) - Vestuário...\n",
      "Treinando modelos para Belem (PA) - Alimentos...\n",
      "Treinando modelos para Belem (PA) - Casa e Decoração...\n",
      "Treinando modelos para Belem (PA) - Beleza e Saúde...\n",
      "Treinando modelos para Belem (PA) - Esportes...\n",
      "Treinando modelos para Belem (PA) - Livros e Mídia...\n",
      "Treinando modelos para Belem (PA) - Brinquedos...\n",
      "Treinando modelos para Recife (PE) - Eletrônicos...\n",
      "Treinando modelos para Recife (PE) - Vestuário...\n",
      "Treinando modelos para Recife (PE) - Alimentos...\n",
      "Treinando modelos para Recife (PE) - Casa e Decoração...\n",
      "Treinando modelos para Recife (PE) - Beleza e Saúde...\n",
      "Treinando modelos para Recife (PE) - Esportes...\n",
      "Treinando modelos para Recife (PE) - Livros e Mídia...\n",
      "Treinando modelos para Recife (PE) - Brinquedos...\n",
      "Treinando modelos para Brasilia (DF) - Eletrônicos...\n",
      "Treinando modelos para Brasilia (DF) - Vestuário...\n",
      "Treinando modelos para Brasilia (DF) - Alimentos...\n",
      "Treinando modelos para Brasilia (DF) - Casa e Decoração...\n",
      "Treinando modelos para Brasilia (DF) - Beleza e Saúde...\n",
      "Treinando modelos para Brasilia (DF) - Esportes...\n",
      "Treinando modelos para Brasilia (DF) - Livros e Mídia...\n",
      "Treinando modelos para Brasilia (DF) - Brinquedos...\n",
      "Treinando modelos para Sao Paulo (SP) - Eletrônicos...\n",
      "Treinando modelos para Sao Paulo (SP) - Vestuário...\n",
      "Treinando modelos para Sao Paulo (SP) - Alimentos...\n",
      "Treinando modelos para Sao Paulo (SP) - Casa e Decoração...\n",
      "Treinando modelos para Sao Paulo (SP) - Beleza e Saúde...\n",
      "Treinando modelos para Sao Paulo (SP) - Esportes...\n",
      "Treinando modelos para Sao Paulo (SP) - Livros e Mídia...\n",
      "Treinando modelos para Sao Paulo (SP) - Brinquedos...\n",
      "Treinando modelos para Florianopolis (SC) - Eletrônicos...\n",
      "Treinando modelos para Florianopolis (SC) - Vestuário...\n",
      "Treinando modelos para Florianopolis (SC) - Alimentos...\n",
      "Treinando modelos para Florianopolis (SC) - Casa e Decoração...\n",
      "Treinando modelos para Florianopolis (SC) - Beleza e Saúde...\n",
      "Treinando modelos para Florianopolis (SC) - Esportes...\n",
      "Treinando modelos para Florianopolis (SC) - Livros e Mídia...\n",
      "Treinando modelos para Florianopolis (SC) - Brinquedos...\n",
      "Treinamento de modelos concluído com sucesso.\n",
      "Gerando previsões para os próximos 30 dias...\n",
      "Previsões geradas com sucesso.\n",
      "Previsões de demanda salvas em 'output/previsoes_demanda.csv'\n",
      "Analisando gargalos e gerando recomendações...\n",
      "Análise de gargalos e recomendações concluídas com sucesso.\n",
      "\n",
      "Pipeline de análise concluído com sucesso!\n",
      "Todos os resultados foram salvos no diretório 'output/'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Função principal\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Função principal que executa todo o pipeline de análise.\n",
    "    \"\"\"\n",
    "    print(\"Iniciando pipeline de análise de demanda logística com machine learning...\")\n",
    "\n",
    "    df = gerar_dados_demanda(dias=365, pedidos_base=50000)\n",
    "\n",
    "    df.to_csv('output/dados_demanda_brutos.csv', index=False)\n",
    "\n",
    "    print(\"Dados brutos salvos em 'output/dados_demanda_brutos.csv'\")\n",
    "\n",
    "    df_prep = preparar_dados_modelagem(df)\n",
    "\n",
    "    df_prep.to_csv('output/dados_demanda_preparados.csv', index=False)\n",
    "\n",
    "    print(\"Dados preparados salvos em 'output/dados_demanda_preparados.csv'\")\n",
    "\n",
    "    insights = analise_exploratoria(df)\n",
    "\n",
    "    # Salvar insights\n",
    "    with open('output/insights_analise.txt', 'w') as f:\n",
    "        for key, value in insights.items():\n",
    "            f.write(f\"{key}: {value}\\n\\n\")\n",
    "    print(\"Insights da análise exploratória salvos em 'output/insights_analise.txt'\")\n",
    "\n",
    "    resultados_modelos = treinar_modelos_previsao(df_prep, horizonte_previsao=30)\n",
    "\n",
    "    df_previsoes = gerar_previsoes_futuras(df_prep, resultados_modelos, dias_futuros=30)\n",
    "\n",
    "    # Salvar previsões\n",
    "    df_previsoes.to_csv('output/previsoes_demanda.csv', index=False)\n",
    "    print(\"Previsões de demanda salvas em 'output/previsoes_demanda.csv'\")\n",
    "\n",
    "    recomendacoes = analisar_gargalos_recomendar(df_previsoes)\n",
    "\n",
    "    print(\"\\nPipeline de análise concluído com sucesso!\")\n",
    "    print(\"Todos os resultados foram salvos no diretório 'output/'\")\n",
    "\n",
    "    return {\n",
    "        'dados_brutos': df,\n",
    "        'dados_preparados': df_prep,\n",
    "        'insights': insights,\n",
    "        'modelos': resultados_modelos,\n",
    "        'previsoes': df_previsoes,\n",
    "        'recomendacoes': recomendacoes\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    resultados = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59042362",
   "metadata": {},
   "source": [
    "# Fim do pipeline que engloba todas as etapas\n",
    "\n",
    "\n",
    "## Arquivos gerados\n",
    "| Nome do Arquivo                                              | Caminho Relativo                                   | Objetivo                                                                                      |\n",
    "|--------------------------------------------------------------|----------------------------------------------------|-----------------------------------------------------------------------------------------------|\n",
    "| Dados de Demanda Brutos                                      | output/dados_demanda_brutos.csv                    | Dados sintéticos brutos gerados para todas as combinações de CD, categoria e data             |\n",
    "| Dados de Demanda Preparados                                  | output/dados_demanda_preparados.csv                | Dados tratados e enriquecidos com features para uso nos modelos de machine learning            |\n",
    "| Insights da Análise Exploratória                             | output/insights_analise.txt                        | Resumo dos principais insights e padrões encontrados na análise exploratória                   |\n",
    "| Previsões de Demanda                                         | output/previsoes_demanda.csv                       | Previsões de demanda futura para cada CD e categoria nos próximos dias                        |\n",
    "| Relatório de Recomendações                                   | output/recomendacoes/relatorio_recomendacoes.md    | Relatório detalhado com análise de gargalos e recomendações operacionais                      |\n",
    "| Gráficos de Análise Exploratória (diversos)                  | output/visualizacoes/*.png                         | Visualizações gráficas dos dados, padrões, correlações, gargalos e tendências                  |\n",
    "| Gráficos de Previsão dos Modelos (um por CD e categoria)     | output/modelos/previsao_{CD}_{Categoria}.png        | Gráficos comparando valores reais e previstos pelos modelos para cada CD e categoria           |\n",
    "| Gráficos de Importância das Features (um por CD e categoria) | output/modelos/importancia_{CD}_{Categoria}.png     | Gráficos mostrando a importância das variáveis para os modelos de previsão                     |\n",
    "| Gráficos de Gargalos e Capacidade Prevista                   | output/recomendacoes/*.png                         | Visualizações dos gargalos previstos e capacidade utilizada para apoiar recomendações           |\n",
    "\n",
    "\n",
    "## Conclusão\n",
    "- A solução de previsão de demanda foi implementada com sucesso, utilizando técnicas avançadas de aprendizado de máquina.\n",
    "- O modelo foi treinado e avaliado com base em dados históricos, considerando variáveis sazonais, operacionais e eventos especiais.\n",
    "- A análise exploratória de dados revelou insights valiosos sobre padrões de demanda, gargalos operacionais e fatores que influenciam a eficiência dos processos logísticos.\n",
    "- A previsão de demanda é uma ferramenta essencial para otimizar a operação logística, reduzir custos e melhorar a experiência do cliente.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
